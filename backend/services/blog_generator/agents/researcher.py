"""
Researcher Agent - ç´ ææ”¶é›†
"""

import json
import logging
import os
from typing import Dict, Any, List, Optional

from ..prompts import get_prompt_manager
from ..services.smart_search_service import get_smart_search_service, init_smart_search_service
from ..utils.cache_utils import get_cache_manager

logger = logging.getLogger(__name__)


class ResearcherAgent:
    """
    ä¸»é¢˜ç´ ææ”¶é›†å¸ˆ - è´Ÿè´£è”ç½‘æœç´¢æ”¶é›†èƒŒæ™¯èµ„æ–™
    æ”¯æŒæ–‡æ¡£çŸ¥è¯†èåˆï¼ˆä¸€æœŸï¼‰
    """
    
    def __init__(self, llm_client, search_service=None, knowledge_service=None):
        """
        åˆå§‹åŒ– Researcher Agent

        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            search_service: æœç´¢æœåŠ¡ (å¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è·³è¿‡æœç´¢)
            knowledge_service: çŸ¥è¯†æœåŠ¡ (å¯é€‰ï¼Œç”¨äºæ–‡æ¡£çŸ¥è¯†èåˆ)
        """
        self.llm = llm_client
        self.search_service = search_service
        self.knowledge_service = knowledge_service

        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        self.cache_enabled = os.environ.get('RESEARCHER_CACHE_ENABLED', 'true').lower() == 'true'
        if self.cache_enabled:
            self.cache = get_cache_manager()
            logger.info("ğŸ’¾ Researcher ç¼“å­˜å·²å¯ç”¨")
        else:
            self.cache = None

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ™ºèƒ½æœç´¢
        self.smart_search_enabled = os.environ.get('SMART_SEARCH_ENABLED', 'false').lower() == 'true'
        if self.smart_search_enabled:
            # åˆå§‹åŒ–æ™ºèƒ½æœç´¢æœåŠ¡
            smart_service = get_smart_search_service()
            if not smart_service:
                init_smart_search_service(llm_client)
            logger.info("ğŸ§  æ™ºèƒ½çŸ¥è¯†æºæœç´¢å·²å¯ç”¨")
    
    def generate_search_queries(self, topic: str, target_audience: str) -> List[str]:
        """
        ç”Ÿæˆæœç´¢æŸ¥è¯¢
        
        Args:
            topic: æŠ€æœ¯ä¸»é¢˜
            target_audience: ç›®æ ‡å—ä¼—
            
        Returns:
            æœç´¢æŸ¥è¯¢åˆ—è¡¨
        """
        # é»˜è®¤æœç´¢ç­–ç•¥
        default_queries = [
            f"{topic} æ•™ç¨‹ tutorial",
            f"{topic} æœ€ä½³å®è·µ best practices",
            f"{topic} å¸¸è§é—®é¢˜ FAQ",
        ]
        
        if not self.llm:
            return default_queries
        
        try:
            pm = get_prompt_manager()
            prompt = pm.render_search_query(
                topic=topic,
                target_audience=target_audience
            )
            
            response = self.llm.chat(
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )
            
            queries = json.loads(response)
            if isinstance(queries, list):
                return queries
            return default_queries
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆæœç´¢æŸ¥è¯¢å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢")
            return default_queries
    
    def search(self, topic: str, target_audience: str, max_results: int = 10) -> List[Dict]:
        """
        æ‰§è¡Œæœç´¢

        Args:
            topic: æŠ€æœ¯ä¸»é¢˜
            target_audience: ç›®æ ‡å—ä¼—
            max_results: æœ€å¤§ç»“æœæ•°

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # å°è¯•ä»ç¼“å­˜è·å–
        if self.cache:
            cached_result = self.cache.get(
                'search',
                topic=topic,
                target_audience=target_audience,
                max_results=max_results
            )
            if cached_result is not None:
                return cached_result

        if not self.search_service:
            logger.warning("æœç´¢æœåŠ¡æœªé…ç½®ï¼Œè·³è¿‡æœç´¢")
            return []

        queries = self.generate_search_queries(topic, target_audience)
        all_results = []

        for query in queries:
            try:
                result = self.search_service.search(query, max_results=max_results // len(queries))
                if result.get('success') and result.get('results'):
                    all_results.extend(result['results'])
            except Exception as e:
                logger.error(f"æœç´¢å¤±è´¥ [{query}]: {e}")

        # å»é‡
        seen_urls = set()
        unique_results = []
        for item in all_results:
            url = item.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(item)

        final_results = unique_results[:max_results]

        # ä¿å­˜åˆ°ç¼“å­˜
        if self.cache:
            self.cache.set(
                'search',
                final_results,
                topic=topic,
                target_audience=target_audience,
                max_results=max_results
            )

        return final_results
    
    def _smart_search(self, topic: str, target_audience: str, max_results: int = 15) -> List[Dict]:
        """
        ä½¿ç”¨æ™ºèƒ½æœç´¢æœåŠ¡ï¼ˆLLM è·¯ç”± + å¤šæºå¹¶è¡Œï¼‰

        Args:
            topic: æŠ€æœ¯ä¸»é¢˜
            target_audience: ç›®æ ‡å—ä¼—
            max_results: æœ€å¤§ç»“æœæ•°

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # å°è¯•ä»ç¼“å­˜è·å–
        if self.cache:
            cached_result = self.cache.get(
                'smart_search',
                topic=topic,
                target_audience=target_audience,
                max_results=max_results
            )
            if cached_result is not None:
                return cached_result

        smart_service = get_smart_search_service()
        if not smart_service:
            logger.warning("æ™ºèƒ½æœç´¢æœåŠ¡æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°æ™®é€šæœç´¢")
            return self.search(topic, target_audience, max_results)

        try:
            result = smart_service.search(
                topic=topic,
                article_type=target_audience,
                max_results_per_source=5
            )

            if result.get('success'):
                sources_used = result.get('sources_used', [])
                logger.info(f"ğŸ§  æ™ºèƒ½æœç´¢å®Œæˆï¼Œä½¿ç”¨æœç´¢æº: {sources_used}")
                search_results = result.get('results', [])[:max_results]

                # ä¿å­˜åˆ°ç¼“å­˜
                if self.cache:
                    self.cache.set(
                        'smart_search',
                        search_results,
                        topic=topic,
                        target_audience=target_audience,
                        max_results=max_results
                    )

                return search_results
            else:
                logger.warning(f"æ™ºèƒ½æœç´¢å¤±è´¥: {result.get('error')}ï¼Œå›é€€åˆ°æ™®é€šæœç´¢")
                return self.search(topic, target_audience, max_results)

        except Exception as e:
            logger.error(f"æ™ºèƒ½æœç´¢å¼‚å¸¸: {e}ï¼Œå›é€€åˆ°æ™®é€šæœç´¢")
            return self.search(topic, target_audience, max_results)
    
    def summarize(
        self,
        topic: str,
        search_results: List[Dict],
        target_audience: str,
        search_depth: str = "medium"
    ) -> Dict[str, Any]:
        """
        æ•´ç†æœç´¢ç»“æœï¼Œç”ŸæˆèƒŒæ™¯çŸ¥è¯†æ‘˜è¦

        Args:
            topic: æŠ€æœ¯ä¸»é¢˜
            search_results: æœç´¢ç»“æœ
            target_audience: ç›®æ ‡å—ä¼—
            search_depth: æœç´¢æ·±åº¦

        Returns:
            æ•´ç†åçš„ç»“æœ
        """
        if not search_results:
            return {
                "background_knowledge": f"å…³äº {topic} çš„èƒŒæ™¯çŸ¥è¯†å°†åœ¨åç»­ç« èŠ‚ä¸­è¯¦ç»†ä»‹ç»ã€‚",
                "key_concepts": [],
                "top_references": []
            }

        # å°è¯•ä»ç¼“å­˜è·å–ï¼ˆåŸºäº topic å’Œæœç´¢ç»“æœçš„ URL åˆ—è¡¨ï¼‰
        if self.cache:
            result_urls = [r.get('url', '') for r in search_results[:10]]
            cached_result = self.cache.get(
                'summarize',
                topic=topic,
                target_audience=target_audience,
                search_depth=search_depth,
                result_urls=result_urls
            )
            if cached_result is not None:
                return cached_result

        pm = get_prompt_manager()
        prompt = pm.render_researcher(
            topic=topic,
            search_depth=search_depth,
            target_audience=target_audience,
            search_results=search_results[:10]
        )

        try:
            response = self.llm.chat(
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )

            # æå– JSONï¼ˆå¤„ç† markdown ä»£ç å—ï¼‰
            json_str = response.strip()
            if '```json' in json_str:
                start = json_str.find('```json') + 7
                end = json_str.find('```', start)
                json_str = json_str[start:end].strip() if end != -1 else json_str[start:].strip()
            elif '```' in json_str:
                start = json_str.find('```') + 3
                end = json_str.find('```', start)
                json_str = json_str[start:end].strip() if end != -1 else json_str[start:].strip()

            # å°è¯•è§£æ JSON
            try:
                result = json.loads(json_str)
            except json.JSONDecodeError:
                result = json.loads(json_str, strict=False)
            key_concepts = result.get("key_concepts", [])

            # è°ƒè¯•ï¼šæ‰“å°å®é™…è¿”å›å†…å®¹
            logger.info(f"LLM è¿”å› key_concepts ç±»å‹: {type(key_concepts)}, å€¼: {key_concepts}")

            # å¦‚æœ key_concepts ä¸ºç©ºä½†æœ‰å…¶ä»–å¯èƒ½çš„å­—æ®µå
            if not key_concepts:
                # å°è¯•å…¶ä»–å¯èƒ½çš„å­—æ®µå
                for alt_key in ['keyConcepts', 'concepts', 'core_concepts', 'keywords']:
                    if result.get(alt_key):
                        key_concepts = result.get(alt_key)
                        logger.info(f"ä½¿ç”¨å¤‡é€‰å­—æ®µ {alt_key}: {key_concepts}")
                        break

            if key_concepts:
                logger.info(f"æ ¸å¿ƒæ¦‚å¿µ: {[c.get('name', c) if isinstance(c, dict) else c for c in key_concepts[:5]]}")

            # è§£æ Instructional Design åˆ†æï¼ˆæ–°å¢ï¼‰
            instructional_analysis = result.get("instructional_analysis", {})
            if instructional_analysis:
                learning_objectives = instructional_analysis.get("learning_objectives", [])
                verbatim_data = instructional_analysis.get("verbatim_data", [])
                content_type = instructional_analysis.get("content_type", "tutorial")
                logger.info(f"ğŸ“š æ•™å­¦è®¾è®¡åˆ†æ: å­¦ä¹ ç›®æ ‡ {len(learning_objectives)} ä¸ª, "
                           f"Verbatim æ•°æ® {len(verbatim_data)} é¡¹, å†…å®¹ç±»å‹: {content_type}")

            summary_result = {
                "background_knowledge": result.get("background_knowledge", ""),
                "key_concepts": key_concepts,
                "top_references": result.get("top_references", []),
                "instructional_analysis": instructional_analysis  # æ–°å¢
            }

            # ä¿å­˜åˆ°ç¼“å­˜
            if self.cache:
                result_urls = [r.get('url', '') for r in search_results[:10]]
                self.cache.set(
                    'summarize',
                    summary_result,
                    topic=topic,
                    target_audience=target_audience,
                    search_depth=search_depth,
                    result_urls=result_urls
                )

            return summary_result

        except json.JSONDecodeError as e:
            logger.error(f"JSON è§£æå¤±è´¥: {e}, å“åº”å†…å®¹: {response[:500] if response else 'None'}")
        except Exception as e:
            logger.error(f"æ•´ç†æœç´¢ç»“æœå¤±è´¥: {e}")

        # è¿”å›ç®€å•æ‘˜è¦
        return {
            "background_knowledge": '\n'.join([
                item.get('content', '')[:200] for item in search_results[:3]
            ]),
            "key_concepts": [],
            "top_references": [
                {"title": item.get('title', ''), "url": item.get('url', '')}
                    for item in search_results[:5]
                ]
            }
    
    def distill(self, topic: str, search_results: List[Dict]) -> Dict[str, Any]:
        """
        æ·±åº¦æç‚¼æœç´¢ç»“æœï¼ˆç±» OpenDraft Scribeï¼‰

        Args:
            topic: æŠ€æœ¯ä¸»é¢˜
            search_results: åŸå§‹æœç´¢ç»“æœ

        Returns:
            æç‚¼åçš„ç»“æ„åŒ–ç´ æ
        """
        empty_result = {
            "sources": [],
            "common_themes": [],
            "contradictions": [],
            "material_by_type": {"concepts": [], "cases": [], "data": [], "comparisons": []}
        }
        if not search_results:
            return empty_result

        # å°è¯•ä»ç¼“å­˜è·å–
        if self.cache:
            result_urls = [r.get('url', '') for r in search_results[:15]]
            cached_result = self.cache.get(
                'distill',
                topic=topic,
                result_urls=result_urls
            )
            if cached_result is not None:
                return cached_result

        pm = get_prompt_manager()
        prompt = pm.render_distill_sources(
            topic=topic,
            search_results=search_results[:15]
        )

        try:
            response = self.llm.chat(
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )

            # æå– JSON
            json_str = response.strip()
            if '```json' in json_str:
                json_str = json_str.split('```json')[1].split('```')[0].strip()
            elif '```' in json_str:
                json_str = json_str.split('```')[1].split('```')[0].strip()

            result = json.loads(json_str)

            # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            result.setdefault('sources', [])
            result.setdefault('common_themes', [])
            result.setdefault('contradictions', [])
            result.setdefault('material_by_type',
                              {"concepts": [], "cases": [], "data": [], "comparisons": []})

            logger.info(f"ğŸ”¬ æ·±åº¦æç‚¼å®Œæˆ: {len(result['sources'])} æ¡ç´ æ, "
                        f"{len(result['common_themes'])} ä¸ªå…±åŒä¸»é¢˜, "
                        f"{len(result['contradictions'])} ä¸ªçŸ›ç›¾ç‚¹")

            # ä¿å­˜åˆ°ç¼“å­˜
            if self.cache:
                result_urls = [r.get('url', '') for r in search_results[:15]]
                self.cache.set(
                    'distill',
                    result,
                    topic=topic,
                    result_urls=result_urls
                )

            return result

        except Exception as e:
            logger.error(f"æ·±åº¦æç‚¼å¤±è´¥: {e}")
            return empty_result

    def analyze_gaps(self, topic: str, article_type: str, distilled: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç¼ºå£åˆ†æï¼ˆç±» OpenDraft Signalï¼‰

        Args:
            topic: æŠ€æœ¯ä¸»é¢˜
            article_type: æ–‡ç« ç±»å‹
            distilled: distill() çš„è¾“å‡º

        Returns:
            ç¼ºå£åˆ†æç»“æœ
        """
        empty_result = {
            "content_gaps": [],
            "unique_angles": [],
            "writing_recommendations": {}
        }
        if not distilled or not distilled.get('sources'):
            return empty_result

        # å°è¯•ä»ç¼“å­˜è·å–
        if self.cache:
            cached_result = self.cache.get(
                'analyze_gaps',
                topic=topic,
                article_type=article_type,
                themes_count=len(distilled.get('common_themes', []))
            )
            if cached_result is not None:
                return cached_result

        pm = get_prompt_manager()
        prompt = pm.render_analyze_gaps(
            topic=topic,
            article_type=article_type,
            common_themes=distilled.get('common_themes', []),
            material_by_type=distilled.get('material_by_type', {}),
            contradictions=distilled.get('contradictions', [])
        )

        try:
            response = self.llm.chat(
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )

            # æå– JSON
            json_str = response.strip()
            if '```json' in json_str:
                json_str = json_str.split('```json')[1].split('```')[0].strip()
            elif '```' in json_str:
                json_str = json_str.split('```')[1].split('```')[0].strip()

            result = json.loads(json_str)

            # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            result.setdefault('content_gaps', [])
            result.setdefault('unique_angles', [])
            result.setdefault('writing_recommendations', {})

            logger.info(f"ğŸ” ç¼ºå£åˆ†æå®Œæˆ: {len(result['content_gaps'])} ä¸ªç¼ºå£, "
                        f"{len(result['unique_angles'])} ä¸ªç‹¬ç‰¹è§’åº¦")

            # ä¿å­˜åˆ°ç¼“å­˜
            if self.cache:
                self.cache.set(
                    'analyze_gaps',
                    result,
                    topic=topic,
                    article_type=article_type,
                    themes_count=len(distilled.get('common_themes', []))
                )

            return result

        except Exception as e:
            logger.error(f"ç¼ºå£åˆ†æå¤±è´¥: {e}")
            return empty_result

    def run(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œç´ ææ”¶é›†

        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        1. æ— æ–‡æ¡£ä¸Šä¼  â†’ åŸæœ‰æµç¨‹ï¼ˆä»…ç½‘ç»œæœç´¢ï¼‰
        2. æœ‰æ–‡æ¡£ä¸Šä¼  â†’ çŸ¥è¯†èåˆæµç¨‹ï¼ˆæ–‡æ¡£ + ç½‘ç»œæœç´¢ï¼‰

        Args:
            state: å…±äº«çŠ¶æ€

        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        topic = state.get('topic', '')
        target_audience = state.get('target_audience', 'intermediate')
        
        # è·å–æ–‡æ¡£çŸ¥è¯†ï¼ˆå¦‚æœæœ‰ä¸Šä¼ æ–‡æ¡£ï¼‰
        document_knowledge = state.get('document_knowledge', [])
        has_document = bool(document_knowledge)
        
        logger.info(f"ğŸ” å¼€å§‹æ”¶é›†ç´ æ: {topic}")
        
        # å±•ç¤ºæ–‡æ¡£çŸ¥è¯†ï¼ˆæ ‡é¢˜ + é¢„è§ˆå†…å®¹åˆ†å¼€ï¼‰
        for doc in document_knowledge[:3]:
            file_name = doc.get('file_name', 'æœªçŸ¥æ–‡æ¡£')
            content = doc.get('content', '')
            # æ ‡é¢˜è¡Œ
            logger.info(f"ğŸ“„ æ–‡æ¡£: {file_name} ({len(content)} å­—)")
            # é¢„è§ˆå†…å®¹ï¼ˆå‰1000å­—ï¼Œä½œä¸ºå•ç‹¬çš„æ—¥å¿—ï¼‰
            preview = content[:1000] + '...' if len(content) > 1000 else content
            logger.info(f"__DOC_PREVIEW__{preview}__END_PREVIEW__")
        
        # 1. æ‰§è¡Œç½‘ç»œæœç´¢
        if self.smart_search_enabled:
            # ä½¿ç”¨æ™ºèƒ½æœç´¢ï¼ˆLLM è·¯ç”± + å¤šæºå¹¶è¡Œï¼‰
            logger.info(f"ğŸ§  å¯åŠ¨æ™ºèƒ½çŸ¥è¯†æºæœç´¢...")
            search_results = self._smart_search(topic, target_audience)
        else:
            # ä½¿ç”¨æ™®é€šæœç´¢
            logger.info(f"ğŸŒ å¯åŠ¨ç½‘ç»œæœç´¢...")
            search_results = self.search(topic, target_audience)
        
        # 2. çŸ¥è¯†èåˆåˆ†æ”¯
        if self.knowledge_service and has_document:
            # âœ… æœ‰æ–‡æ¡£ â†’ èµ°çŸ¥è¯†èåˆé€»è¾‘
            logger.info("ä½¿ç”¨çŸ¥è¯†èåˆæ¨¡å¼")
            
            # å°†æ–‡æ¡£çŸ¥è¯†è½¬æ¢ä¸º KnowledgeItem
            doc_items = self.knowledge_service.prepare_document_knowledge(
                [{'filename': d.get('file_name', ''), 'markdown_content': d.get('content', '')} 
                 for d in document_knowledge]
            )
            
            # å°†æœç´¢ç»“æœè½¬æ¢ä¸º KnowledgeItem
            web_items = self.knowledge_service.convert_search_results(search_results)
            
            # èåˆçŸ¥è¯†
            merged_knowledge = self.knowledge_service.get_merged_knowledge(
                document_knowledge=doc_items,
                web_knowledge=web_items
            )
            
            # æ•´ç†ä¸º Prompt å¯ç”¨æ ¼å¼
            summary = self.knowledge_service.summarize_for_prompt(merged_knowledge)
            
            # è®°å½•çŸ¥è¯†æ¥æºç»Ÿè®¡
            state['knowledge_source_stats'] = {
                'document_count': len([k for k in merged_knowledge if k.source_type == 'document']),
                'web_count': len([k for k in merged_knowledge if k.source_type == 'web_search']),
                'total_items': len(merged_knowledge)
            }
            state['document_references'] = summary.get('document_references', [])
            
        else:
            # âœ… æ— æ–‡æ¡£ â†’ å®Œå…¨èµ°åŸæœ‰é€»è¾‘ï¼Œé›¶æ”¹åŠ¨
            logger.info("ğŸ“‹ ä½¿ç”¨åŸæœ‰æœç´¢æ¨¡å¼ï¼ˆæ— æ–‡æ¡£ä¸Šä¼ ï¼‰")
            logger.info(f"ğŸ“‹ å°†ä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœç”Ÿæˆåšå®¢å†…å®¹")
            summary = self.summarize(
                topic=topic,
                search_results=search_results,
                target_audience=target_audience
            )
            state['knowledge_source_stats'] = {
                'document_count': 0,
                'web_count': len(search_results),
                'total_items': len(search_results)
            }
            state['document_references'] = []
        
        # 3. æ›´æ–°çŠ¶æ€
        state['search_results'] = search_results
        state['background_knowledge'] = summary.get('background_knowledge', '')
        state['key_concepts'] = [
            c.get('name', c) if isinstance(c, dict) else c
            for c in summary.get('key_concepts', [])
        ]
        # ä¿ç•™å®Œæ•´çš„å¼•ç”¨ä¿¡æ¯ï¼ˆåŒ…å« title å’Œ urlï¼‰
        state['reference_links'] = [
            r if isinstance(r, dict) else {'title': '', 'url': r}
            for r in summary.get('top_references', summary.get('web_references', []))
        ]
        
        # 4. æ›´æ–° Instructional Design ç›¸å…³çŠ¶æ€ï¼ˆæ–°å¢ï¼‰
        instructional_analysis = summary.get('instructional_analysis', {})
        state['instructional_analysis'] = instructional_analysis
        state['learning_objectives'] = instructional_analysis.get('learning_objectives', [])
        state['verbatim_data'] = instructional_analysis.get('verbatim_data', [])

        # 5. æ·±åº¦æç‚¼ + ç¼ºå£åˆ†æï¼ˆ52å·æ–¹æ¡ˆï¼‰
        distilled = {}
        gap_analysis = {}
        if search_results:
            logger.info("ğŸ”¬ å¼€å§‹æ·±åº¦æç‚¼æœç´¢ç»“æœ...")
            distilled = self.distill(topic, search_results)

            logger.info("ğŸ” å¼€å§‹ç¼ºå£åˆ†æ...")
            article_type = state.get('article_type', 'tutorial')
            gap_analysis = self.analyze_gaps(topic, article_type, distilled)

        state['distilled_sources'] = distilled.get('sources', [])
        state['material_by_type'] = distilled.get('material_by_type', {})
        state['common_themes'] = distilled.get('common_themes', [])
        state['contradictions'] = distilled.get('contradictions', [])
        state['content_gaps'] = gap_analysis.get('content_gaps', [])
        state['unique_angles'] = gap_analysis.get('unique_angles', [])
        state['writing_recommendations'] = gap_analysis.get('writing_recommendations', {})

        stats = state['knowledge_source_stats']
        logger.info(f"âœ… ç´ ææ”¶é›†å®Œæˆ: æ–‡æ¡£çŸ¥è¯† {stats['document_count']} æ¡, "
                    f"ç½‘ç»œæœç´¢ {stats['web_count']} æ¡, æ ¸å¿ƒæ¦‚å¿µ {len(state['key_concepts'])} ä¸ª")
        
        # æ‰“å° Instructional Design ç»Ÿè®¡
        if instructional_analysis:
            logger.info(f"ğŸ“š æ•™å­¦è®¾è®¡: å­¦ä¹ ç›®æ ‡ {len(state['learning_objectives'])} ä¸ª, "
                       f"Verbatim æ•°æ® {len(state['verbatim_data'])} é¡¹")
        
        # è¾“å‡º researcher é˜¶æ®µç»“æœï¼ˆç”¨äºæµ‹è¯• mockï¼‰
        import json
        researcher_output = {
            'background_knowledge': state.get('background_knowledge', ''),
            'key_concepts': state.get('key_concepts', []),
            'reference_links': state.get('reference_links', []),
            'learning_objectives': state.get('learning_objectives', []),
            'verbatim_data': state.get('verbatim_data', []),
            'knowledge_source_stats': state.get('knowledge_source_stats', {}),
            'distilled_sources': state.get('distilled_sources', []),
            'content_gaps': state.get('content_gaps', []),
            'writing_recommendations': state.get('writing_recommendations', {}),
        }
        logger.info(f"__RESEARCHER_OUTPUT_JSON__{json.dumps(researcher_output, ensure_ascii=False)}__END_JSON__")
        
        return state
