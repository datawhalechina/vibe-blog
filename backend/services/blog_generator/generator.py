"""
é•¿æ–‡åšå®¢ç”Ÿæˆå™¨ - LangGraph å·¥ä½œæµä¸»å…¥å£
"""

import logging
import os
from typing import Dict, Any, Optional, Literal, Callable


def _should_use_parallel():
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å¹¶è¡Œæ‰§è¡Œã€‚å½“å¼€å¯è¿½è¸ªæ—¶ç¦ç”¨å¹¶è¡Œï¼Œé¿å…ä¸Šä¸‹æ–‡ä¸¢å¤±ã€‚"""
    if os.environ.get('TRACE_ENABLED', 'false').lower() == 'true':
        return False
    return True

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import interrupt

from .schemas.state import SharedState, create_initial_state
from .style_profile import StyleProfile
from .agents.researcher import ResearcherAgent
from .agents.planner import PlannerAgent
from .agents.writer import WriterAgent
from .agents.coder import CoderAgent
from .agents.artist import ArtistAgent
from .agents.questioner import QuestionerAgent
from .agents.reviewer import ReviewerAgent
from .agents.assembler import AssemblerAgent
from .agents.search_coordinator import SearchCoordinator
from .agents.humanizer import HumanizerAgent
from utils.session_tracker import SessionTracker
from .agents.thread_checker import ThreadCheckerAgent
from .agents.voice_checker import VoiceCheckerAgent
from .agents.factcheck import FactCheckAgent
from .agents.summary_generator import SummaryGeneratorAgent

logger = logging.getLogger(__name__)


def _get_content_word_count(state: Dict[str, Any]) -> int:
    """è®¡ç®—å½“å‰ state ä¸­æ‰€æœ‰ç« èŠ‚å†…å®¹çš„æ€»å­—æ•°"""
    sections = state.get('sections', [])
    total = 0
    for section in sections:
        content = section.get('content', '')
        if content:
            total += len(content)
    return total


def _log_word_count_diff(agent_name: str, before: int, after: int):
    """è®°å½•å­—æ•°å˜åŒ–çš„ diff"""
    diff = after - before
    if diff >= 0:
        logger.info(f"ğŸ“Š [{agent_name}] å­—æ•°å˜åŒ–: {before} â†’ {after} (+{diff} å­—)")
    else:
        logger.info(f"ğŸ“Š [{agent_name}] å­—æ•°å˜åŒ–: {before} â†’ {after} ({diff} å­—)")


class BlogGenerator:
    """
    é•¿æ–‡åšå®¢ç”Ÿæˆå™¨
    
    åŸºäº LangGraph å®ç°çš„ Multi-Agent ååŒç”Ÿæˆç³»ç»Ÿ
    """
    
    def __init__(
        self,
        llm_client,
        search_service=None,
        knowledge_service=None,
        max_questioning_rounds: int = 2,
        max_revision_rounds: int = 3,
        style: StyleProfile = None
    ):
        """
        åˆå§‹åŒ–åšå®¢ç”Ÿæˆå™¨

        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            search_service: æœç´¢æœåŠ¡ (å¯é€‰)
            knowledge_service: çŸ¥è¯†æœåŠ¡ (å¯é€‰ï¼Œç”¨äºæ–‡æ¡£çŸ¥è¯†èåˆ)
            max_questioning_rounds: æœ€å¤§è¿½é—®è½®æ•°
            max_revision_rounds: æœ€å¤§ä¿®è®¢è½®æ•°
            style: é£æ ¼é…ç½®ï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™ä»ç¯å¢ƒå˜é‡æ„å»ºé»˜è®¤å€¼ï¼‰
        """
        self.llm = llm_client
        self.search_service = search_service
        self.knowledge_service = knowledge_service
        self.max_questioning_rounds = max_questioning_rounds
        self.style = style  # å»¶è¿Ÿåˆå§‹åŒ–ï¼šgenerate() æ—¶æ ¹æ® target_length ç¡®å®š

        # max_revision_rounds å‘åå…¼å®¹ï¼šä¼˜å…ˆç”¨ StyleProfileï¼Œå¦åˆ™ç”¨å‚æ•°
        self.max_revision_rounds = max_revision_rounds

        # åˆå§‹åŒ–å„ Agent
        self.researcher = ResearcherAgent(llm_client, search_service, knowledge_service)
        self.planner = PlannerAgent(llm_client)
        self.writer = WriterAgent(llm_client)
        self.coder = CoderAgent(llm_client)
        self.artist = ArtistAgent(llm_client)
        self.questioner = QuestionerAgent(llm_client)
        self.reviewer = ReviewerAgent(llm_client)
        self.assembler = AssemblerAgent()
        self.search_coordinator = SearchCoordinator(llm_client, search_service)

        # å¢å¼º Agentï¼šç¯å¢ƒå˜é‡ä½œä¸ºå…¨å±€å¼€å…³ï¼ˆStyleProfile ä½œä¸ºè¿è¡Œæ—¶å¼€å…³ï¼‰
        self._env_humanizer = os.getenv('HUMANIZER_ENABLED', 'true').lower() == 'true'
        self._env_thread_check = os.getenv('THREAD_CHECK_ENABLED', 'true').lower() == 'true'
        self._env_voice_check = os.getenv('VOICE_CHECK_ENABLED', 'true').lower() == 'true'
        self._env_factcheck = os.getenv('FACTCHECK_ENABLED', 'true').lower() == 'true'
        self._env_text_cleanup = os.getenv('TEXT_CLEANUP_ENABLED', 'true').lower() == 'true'
        self._env_summary = os.getenv('SUMMARY_GENERATOR_ENABLED', 'true').lower() == 'true'

        # åˆå§‹åŒ–å¢å¼º Agentï¼ˆåªè¦ç¯å¢ƒå˜é‡æ²¡ç¦ç”¨å°±åˆ›å»ºå®ä¾‹ï¼‰
        self.humanizer = HumanizerAgent(llm_client) if self._env_humanizer else None
        self.thread_checker = ThreadCheckerAgent(llm_client) if self._env_thread_check else None
        self.voice_checker = VoiceCheckerAgent(llm_client) if self._env_voice_check else None
        self.factcheck = FactCheckAgent(llm_client) if self._env_factcheck else None

        # ä¸šåŠ¡çº§çŠ¶æ€è¿½è¸ªï¼ˆ69.05ï¼‰
        self.tracker = SessionTracker()
        self.summary_generator = SummaryGeneratorAgent(llm_client) if self._env_summary else None

        # 37.12 åˆ†å±‚æ¶æ„æ ¡éªŒå™¨ï¼ˆå¯é€‰ï¼‰
        self._layer_validator = None
        if os.environ.get('LAYER_VALIDATION_ENABLED', 'false').lower() == 'true':
            try:
                from .orchestrator.layer_definitions import BLOG_LAYERS, LayerValidator
                self._layer_validator = LayerValidator(BLOG_LAYERS)
                logger.info("ğŸ—ï¸ åˆ†å±‚æ¶æ„æ ¡éªŒå·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"åˆ†å±‚æ¶æ„æ ¡éªŒåˆå§‹åŒ–å¤±è´¥: {e}")

        # æ„å»ºå·¥ä½œæµ
        self.workflow = self._build_workflow()
        self.app = None

    def _validate_layer(self, layer_name: str, state: Dict[str, Any]):
        """37.12 å±‚é—´æ•°æ®å¥‘çº¦æ ¡éªŒï¼ˆä»…æ—¥å¿—è­¦å‘Šï¼Œä¸é˜»æ–­æµç¨‹ï¼‰"""
        if not self._layer_validator:
            return
        try:
            ok, missing = self._layer_validator.validate_inputs(layer_name, state)
            if not ok:
                logger.warning(f"ğŸ—ï¸ [{layer_name}] å±‚è¾“å…¥ç¼ºå¤±: {missing}")
        except Exception as e:
            logger.debug(f"å±‚æ ¡éªŒå¼‚å¸¸: {e}")
    
    def _build_workflow(self) -> StateGraph:
        """
        æ„å»º LangGraph å·¥ä½œæµ
        
        Returns:
            StateGraph å®ä¾‹
        """
        workflow = StateGraph(SharedState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("researcher", self._researcher_node)
        workflow.add_node("planner", self._planner_node)
        workflow.add_node("writer", self._writer_node)
        # å¤šè½®æœç´¢ç›¸å…³èŠ‚ç‚¹
        workflow.add_node("check_knowledge", self._check_knowledge_node)
        workflow.add_node("refine_search", self._refine_search_node)
        workflow.add_node("enhance_with_knowledge", self._enhance_with_knowledge_node)
        # è¿½é—®å’Œå®¡æ ¸èŠ‚ç‚¹
        workflow.add_node("questioner", self._questioner_node)
        workflow.add_node("deepen_content", self._deepen_content_node)
        workflow.add_node("coder_and_artist", self._coder_and_artist_node)  # å¹¶è¡ŒèŠ‚ç‚¹
        workflow.add_node("section_evaluate", self._section_evaluate_node)  # æ®µè½è¯„ä¼°
        workflow.add_node("section_improve", self._section_improve_node)  # æ®µè½æ”¹è¿›
        workflow.add_node("consistency_check", self._consistency_check_node)  # ä¸€è‡´æ€§æ£€æŸ¥
        workflow.add_node("reviewer", self._reviewer_node)
        workflow.add_node("revision", self._revision_node)
        workflow.add_node("factcheck", self._factcheck_node)
        workflow.add_node("text_cleanup", self._text_cleanup_node)
        workflow.add_node("humanizer", self._humanizer_node)
        workflow.add_node("assembler", self._assembler_node)
        workflow.add_node("summary_generator", self._summary_generator_node)
        
        # å®šä¹‰è¾¹
        workflow.add_edge(START, "researcher")
        workflow.add_edge("researcher", "planner")
        workflow.add_edge("planner", "writer")
        
        # Writer åè¿›å…¥çŸ¥è¯†ç©ºç™½æ£€æŸ¥
        workflow.add_edge("writer", "check_knowledge")
        
        # æ¡ä»¶è¾¹ï¼šæ£€æŸ¥åå†³å®šæ˜¯æœç´¢è¿˜æ˜¯ç»§ç»­åˆ° Questioner
        workflow.add_conditional_edges(
            "check_knowledge",
            self._should_refine_search,
            {
                "search": "refine_search",
                "continue": "questioner"
            }
        )
        
        # æœç´¢åå¢å¼ºå†…å®¹ï¼Œç„¶åå›åˆ°çŸ¥è¯†æ£€æŸ¥
        workflow.add_edge("refine_search", "enhance_with_knowledge")
        workflow.add_edge("enhance_with_knowledge", "check_knowledge")
        
        # æ¡ä»¶è¾¹ï¼šè¿½é—®åå†³å®šæ˜¯æ·±åŒ–è¿˜æ˜¯ç»§ç»­
        workflow.add_conditional_edges(
            "questioner",
            self._should_deepen,
            {
                "deepen": "deepen_content",
                "continue": "section_evaluate"  # è¿›å…¥æ®µè½è¯„ä¼°
            }
        )
        workflow.add_edge("deepen_content", "questioner")  # æ·±åŒ–åé‡æ–°è¿½é—®

        # æ®µè½è¯„ä¼° â†’ æ¡ä»¶è¾¹ï¼šéœ€è¦æ”¹è¿›åˆ™è¿›å…¥æ”¹è¿›èŠ‚ç‚¹ï¼Œå¦åˆ™è·³è¿‡
        workflow.add_conditional_edges(
            "section_evaluate",
            self._should_improve_sections,
            {
                "improve": "section_improve",
                "continue": "coder_and_artist",
            }
        )
        workflow.add_edge("section_improve", "section_evaluate")  # æ”¹è¿›åé‡æ–°è¯„ä¼°
        
        # Coder å’Œ Artist å¹¶è¡Œæ‰§è¡Œï¼ˆé€šè¿‡å•ä¸ªèŠ‚ç‚¹å†…éƒ¨å¹¶è¡Œå®ç°ï¼‰
        workflow.add_edge("coder_and_artist", "consistency_check")
        workflow.add_edge("consistency_check", "reviewer")
        
        # æ¡ä»¶è¾¹ï¼šå®¡æ ¸åå†³å®šæ˜¯ä¿®è®¢è¿˜æ˜¯è¿›å…¥å» AI å‘³
        workflow.add_conditional_edges(
            "reviewer",
            self._should_revise,
            {
                "revision": "revision",
                "assemble": "factcheck"
            }
        )
        workflow.add_edge("revision", "reviewer")  # ä¿®è®¢åé‡æ–°å®¡æ ¸
        workflow.add_edge("factcheck", "text_cleanup")  # äº‹å®æ ¸æŸ¥åæ–‡æœ¬æ¸…ç†
        workflow.add_edge("text_cleanup", "humanizer")  # æ–‡æœ¬æ¸…ç†åå» AI å‘³
        workflow.add_edge("humanizer", "assembler")  # å» AI å‘³åç»„è£…
        workflow.add_edge("assembler", "summary_generator")
        workflow.add_edge("summary_generator", END)
        
        return workflow
    
    def _researcher_node(self, state: SharedState) -> SharedState:
        """ç´ ææ”¶é›†èŠ‚ç‚¹"""
        if state.get('skip_researcher'):
            logger.info("=== Step 1: ç´ ææ”¶é›†ï¼ˆå·²è·³è¿‡ï¼‰ ===")
            return state
        logger.info("=== Step 1: ç´ ææ”¶é›† ===")
        self._validate_layer("research", state)
        return self.researcher.run(state)
    
    def _planner_node(self, state: SharedState) -> SharedState:
        """å¤§çº²è§„åˆ’èŠ‚ç‚¹"""
        logger.info("=== Step 2: å¤§çº²è§„åˆ’ ===")
        self._validate_layer("structure", state)
        # ä½¿ç”¨å®ä¾‹å˜é‡ä¸­çš„æµå¼å›è°ƒ
        on_stream = getattr(self, '_outline_stream_callback', None)
        result = self.planner.run(state, on_stream=on_stream)

        # äº¤äº’å¼æ¨¡å¼ï¼šä½¿ç”¨ LangGraph åŸç”Ÿ interrupt æš‚åœå›¾æ‰§è¡Œ
        outline = result.get('outline') if isinstance(result, dict) else None
        if outline and getattr(self, '_interactive', False):
            sections = outline.get('sections', [])
            interrupt_data = {
                "type": "confirm_outline",
                "title": outline.get("title", ""),
                "sections": sections,
                "sections_titles": [s.get("title", "") for s in sections],
                "narrative_mode": outline.get("narrative_mode", ""),
                "narrative_flow": outline.get("narrative_flow", {}),
                "sections_narrative_roles": [s.get("narrative_role", "") for s in sections],
            }
            user_decision = interrupt(interrupt_data)

            # å¤„ç†ç”¨æˆ·å†³ç­–
            if isinstance(user_decision, dict) and user_decision.get("action") == "edit":
                edited_outline = user_decision.get("outline", outline)
                logger.info(f"å¤§çº²å·²è¢«ç”¨æˆ·ä¿®æ”¹: {edited_outline.get('title', '')}")
                result['outline'] = edited_outline
                result['sections'] = []  # æ¸…ç©ºå·²æœ‰ç« èŠ‚ï¼Œé‡æ–°å†™ä½œ
            else:
                logger.info("å¤§çº²å·²è¢«ç”¨æˆ·ç¡®è®¤")

        return result
    
    def _writer_node(self, state: SharedState) -> SharedState:
        """å†…å®¹æ’°å†™èŠ‚ç‚¹"""
        logger.info("=== Step 3: å†…å®¹æ’°å†™ ===")
        self._validate_layer("content", state)
        before_count = _get_content_word_count(state)
        result = self.writer.run(state)
        after_count = _get_content_word_count(result)
        _log_word_count_diff("Writer", before_count, after_count)
        # åˆå§‹åŒ–ç´¯ç§¯çŸ¥è¯†ï¼ˆé¦–æ¬¡å†™ä½œåï¼‰
        if not result.get('accumulated_knowledge'):
            result['accumulated_knowledge'] = result.get('background_knowledge', '')
        return result
    
    def _check_knowledge_node(self, state: SharedState) -> SharedState:
        """çŸ¥è¯†ç©ºç™½æ£€æŸ¥èŠ‚ç‚¹"""
        search_count = state.get('search_count', 0)
        max_count = state.get('max_search_count', 5)
        logger.info(f"=== Step 3.5: çŸ¥è¯†ç©ºç™½æ£€æŸ¥ (æœç´¢æ¬¡æ•°: {search_count}/{max_count}) ===")
        return self.search_coordinator.run(state)
    
    def _refine_search_node(self, state: SharedState) -> SharedState:
        """ç»†åŒ–æœç´¢èŠ‚ç‚¹"""
        search_count = state.get('search_count', 0) + 1
        max_count = state.get('max_search_count', 5)
        logger.info(f"=== Step 3.6: ç»†åŒ–æœç´¢ (ç¬¬ {search_count} è½®) ===")
        
        gaps = state.get('knowledge_gaps', [])
        result = self.search_coordinator.refine_search(gaps, state)
        
        if result.get('success'):
            logger.info(f"ç»†åŒ–æœç´¢å®Œæˆ: è·å– {len(result.get('results', []))} æ¡ç»“æœ")
        else:
            logger.warning(f"ç»†åŒ–æœç´¢å¤±è´¥: {result.get('reason', 'æœªçŸ¥åŸå› ')}")
        
        return state
    
    def _enhance_with_knowledge_node(self, state: SharedState) -> SharedState:
        """åŸºäºæ–°çŸ¥è¯†å¢å¼ºå†…å®¹èŠ‚ç‚¹ï¼ˆå¹¶è¡Œï¼‰"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import os
        
        logger.info("=== Step 3.7: çŸ¥è¯†å¢å¼º ===")
        
        sections = state.get('sections', [])
        gaps = state.get('knowledge_gaps', [])
        new_knowledge = state.get('accumulated_knowledge', '')
        
        if not gaps or not new_knowledge:
            logger.info("æ²¡æœ‰éœ€è¦å¢å¼ºçš„å†…å®¹")
            return state
        
        from .prompts import get_prompt_manager
        pm = get_prompt_manager()
        
        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†éœ€è¦å¢å¼ºçš„ä»»åŠ¡
        tasks = []
        for section in sections:
            section_gaps = [g for g in gaps if not g.get('section_id') or g.get('section_id') == section.get('id')]
            
            if section_gaps:
                tasks.append({
                    'section': section,
                    'section_gaps': section_gaps,
                    'new_knowledge': new_knowledge
                })
        
        if not tasks:
            logger.info("æ²¡æœ‰éœ€è¦å¢å¼ºçš„ç« èŠ‚")
            state['knowledge_gaps'] = []
            return state
        
        max_workers = int(os.environ.get('BLOG_GENERATOR_MAX_WORKERS', '3'))
        logger.info(f"å¼€å§‹çŸ¥è¯†å¢å¼º: {len(tasks)} ä¸ªç« èŠ‚ï¼Œä½¿ç”¨ {min(max_workers, len(tasks))} ä¸ªå¹¶è¡Œçº¿ç¨‹")
        
        # ç¬¬äºŒæ­¥ï¼šå¹¶è¡Œå¢å¼º
        def enhance_task(task):
            """å•ä¸ªç« èŠ‚å¢å¼ºä»»åŠ¡"""
            section = task['section']
            try:
                prompt = pm.render_writer_enhance_with_knowledge(
                    original_content=section.get('content', ''),
                    new_knowledge=task['new_knowledge'],
                    knowledge_gaps=task['section_gaps']
                )
                
                enhanced_content = self.writer.llm.chat(
                    messages=[{"role": "user", "content": prompt}]
                )
                return {
                    'success': True,
                    'section': section,
                    'enhanced_content': enhanced_content
                }
            except Exception as e:
                logger.error(f"ç« èŠ‚å¢å¼ºå¤±è´¥ [{section.get('title', '')}]: {e}")
                return {
                    'success': False,
                    'section': section,
                    'error': str(e)
                }
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(enhance_task, task): task for task in tasks}
            
            for future in as_completed(futures):
                result = future.result()
                if result['success']:
                    result['section']['content'] = result['enhanced_content']
                    logger.info(f"ç« èŠ‚å¢å¼ºå®Œæˆ: {result['section'].get('title', '')}")
        
        enhanced_count = sum(1 for t in tasks if t['section'].get('content'))
        logger.info(f"çŸ¥è¯†å¢å¼ºå®Œæˆ: {enhanced_count} ä¸ªç« èŠ‚")
        
        # æ¸…ç©ºå·²å¤„ç†çš„çŸ¥è¯†ç©ºç™½
        state['knowledge_gaps'] = []
        
        return state
    
    
    
    def _questioner_node(self, state: SharedState) -> SharedState:
        """è¿½é—®æ£€æŸ¥èŠ‚ç‚¹"""
        logger.info("=== Step 4: è¿½é—®æ£€æŸ¥ ===")
        return self.questioner.run(state)
    
    def _deepen_content_node(self, state: SharedState) -> SharedState:
        """å†…å®¹æ·±åŒ–èŠ‚ç‚¹ï¼ˆå¹¶è¡Œï¼‰"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import os
        
        logger.info("=== Step 4.1: å†…å®¹æ·±åŒ– ===")
        before_count = _get_content_word_count(state)
        state['questioning_count'] = state.get('questioning_count', 0) + 1
        
        # ç»Ÿè®¡éœ€è¦æ·±åŒ–çš„ç« èŠ‚
        sections_to_deepen = [
            r for r in state.get('question_results', [])
            if not r.get('is_detailed_enough', True)
        ]
        total_to_deepen = len(sections_to_deepen)
        
        if total_to_deepen == 0:
            logger.info("æ²¡æœ‰éœ€è¦æ·±åŒ–çš„ç« èŠ‚")
            return state
        
        max_workers = int(os.environ.get('BLOG_GENERATOR_MAX_WORKERS', '3'))
        use_parallel = _should_use_parallel()
        
        if use_parallel:
            logger.info(f"å¼€å§‹æ·±åŒ– {total_to_deepen} ä¸ªç« èŠ‚ï¼Œä½¿ç”¨ {min(max_workers, total_to_deepen)} ä¸ªå¹¶è¡Œçº¿ç¨‹")
        else:
            logger.info(f"å¼€å§‹æ·±åŒ– {total_to_deepen} ä¸ªç« èŠ‚ï¼Œä½¿ç”¨ä¸²è¡Œæ¨¡å¼ï¼ˆè¿½è¸ªå·²å¯ç”¨ï¼‰")
        
        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
        tasks = []
        for idx, result in enumerate(sections_to_deepen, 1):
            section_id = result.get('section_id', '')
            vague_points = result.get('vague_points', [])
            
            # æ‰¾åˆ°å¯¹åº”ç« èŠ‚
            for section in state.get('sections', []):
                if section.get('id') == section_id:
                    tasks.append({
                        'order_idx': idx - 1,
                        'section': section,
                        'section_id': section_id,
                        'vague_points': vague_points,
                        'progress_info': f"[{idx}/{total_to_deepen}]"
                    })
                    break
        
        # æ‰§è¡Œæ·±åŒ–
        def enhance_single_task(task):
            """å•ä¸ªç« èŠ‚æ·±åŒ–ä»»åŠ¡"""
            try:
                section = task['section']
                section_title = section.get('title', task['section_id'])
                original_length = len(section.get('content', ''))
                
                enhanced_content = self.writer.enhance_section(
                    original_content=section.get('content', ''),
                    vague_points=task['vague_points'],
                    section_title=section_title,
                    progress_info=task['progress_info']
                )
                
                new_length = len(enhanced_content)
                logger.info(f"ç« èŠ‚æ·±åŒ–å®Œæˆ: {section_title} (+{new_length - original_length} å­—)")
                
                return {
                    'success': True,
                    'section_id': task['section_id'],
                    'enhanced_content': enhanced_content
                }
            except Exception as e:
                logger.error(f"ç« èŠ‚æ·±åŒ–å¤±è´¥ [{task['section_id']}]: {e}")
                return {
                    'success': False,
                    'section_id': task['section_id'],
                    'enhanced_content': None
                }
        
        if use_parallel:
            # å¹¶è¡Œæ‰§è¡Œ
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(enhance_single_task, task): task for task in tasks}
                
                for future in as_completed(futures):
                    result = future.result()
                    if result['success']:
                        for section in state.get('sections', []):
                            if section.get('id') == result['section_id']:
                                section['content'] = result['enhanced_content']
                                break
        else:
            # ä¸²è¡Œæ‰§è¡Œï¼ˆè¿½è¸ªæ¨¡å¼ï¼‰- ç›´æ¥è°ƒç”¨æ–¹æ³•ä»¥ä¿æŒ Langfuse ä¸Šä¸‹æ–‡
            for task in tasks:
                try:
                    section = task['section']
                    section_title = section.get('title', task['section_id'])
                    original_length = len(section.get('content', ''))
                    
                    enhanced_content = self.writer.enhance_section(
                        original_content=section.get('content', ''),
                        vague_points=task['vague_points'],
                        section_title=section_title,
                        progress_info=task['progress_info']
                    )
                    
                    new_length = len(enhanced_content)
                    logger.info(f"ç« èŠ‚æ·±åŒ–å®Œæˆ: {section_title} (+{new_length - original_length} å­—)")
                    
                    for s in state.get('sections', []):
                        if s.get('id') == task['section_id']:
                            s['content'] = enhanced_content
                            break
                except Exception as e:
                    logger.error(f"ç« èŠ‚æ·±åŒ–å¤±è´¥ [{task['section_id']}]: {e}")
        
        after_count = _get_content_word_count(state)
        _log_word_count_diff("å†…å®¹æ·±åŒ–", before_count, after_count)

        # 69.05: è®°å½•æ·±åŒ–è¿­ä»£å¿«ç…§
        self.tracker.log_deepen_snapshot(
            round_num=state.get('questioning_count', 0),
            sections_deepened=total_to_deepen,
            chars_added=after_count - before_count,
        )

        return state

    # ========== æ®µè½çº§ Generator-Critic Loop (#69.04) ==========

    def _section_evaluate_node(self, state: SharedState) -> SharedState:
        """æ®µè½å¤šç»´åº¦è¯„ä¼°èŠ‚ç‚¹ï¼ˆCritic è§’è‰²ï¼‰"""
        # åŒå¼€å…³ï¼šç¯å¢ƒå˜é‡ + StyleProfile
        style = self._get_style(state)
        if not self._is_enabled("SECTION_EVAL_ENABLED", getattr(style, "enable_thread_check", True)):
            logger.info("æ®µè½è¯„ä¼°å·²ç¦ç”¨ï¼Œè·³è¿‡")
            state["section_evaluations"] = []
            state["needs_section_improvement"] = False
            return state

        logger.info("=== Step 4.2: æ®µè½å¤šç»´åº¦è¯„ä¼° ===")
        sections = state.get("sections", [])
        evaluations = []
        needs_improvement = False

        for i, section in enumerate(sections):
            prev_summary = sections[i - 1].get("title", "") if i > 0 else ""
            next_preview = sections[i + 1].get("title", "") if i < len(sections) - 1 else ""

            evaluation = self.questioner.evaluate_section(
                section_content=section.get("content", ""),
                section_title=section.get("title", ""),
                prev_summary=prev_summary,
                next_preview=next_preview,
            )
            evaluation["section_idx"] = i
            evaluations.append(evaluation)

            if evaluation["overall_quality"] < 7.0:
                needs_improvement = True
                logger.info(
                    f"  æ®µè½ [{section.get('title', '')}] éœ€æ”¹è¿›: "
                    f"overall={evaluation['overall_quality']}"
                )

        state["section_evaluations"] = evaluations
        state["needs_section_improvement"] = needs_improvement

        avg_score = (
            sum(e["overall_quality"] for e in evaluations) / max(len(evaluations), 1)
        )
        logger.info(f"æ®µè½è¯„ä¼°å®Œæˆ: å¹³å‡åˆ† {avg_score:.1f}, éœ€æ”¹è¿›={needs_improvement}")

        # 69.05: è®°å½•æ®µè½è¯„ä¼°åˆ†æ•°
        for evaluation in evaluations:
            self.tracker.log_section_evaluation(
                section_title=sections[evaluation.get("section_idx", 0)].get("title", ""),
                scores=evaluation.get("scores", {}),
                overall=evaluation["overall_quality"],
            )

        return state

    def _should_improve_sections(self, state: SharedState) -> str:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ®µè½çº§æ”¹è¿›"""
        if not state.get("needs_section_improvement", False):
            return "continue"

        improve_count = state.get("section_improve_count", 0)
        if improve_count >= 2:
            logger.info("æ®µè½æ”¹è¿›è¾¾åˆ°æœ€å¤§è½®æ•°(2)ï¼Œè·³è¿‡")
            return "continue"

        # æ”¶æ•›æ£€æµ‹ï¼šæ”¹è¿›å¹…åº¦ < 0.3 åˆ™åœæ­¢
        evaluations = state.get("section_evaluations", [])
        curr_avg = (
            sum(e["overall_quality"] for e in evaluations) / max(len(evaluations), 1)
        )
        prev_avg = state.get("prev_section_avg_score", 0)
        if prev_avg > 0 and (curr_avg - prev_avg) < 0.3:
            logger.info(f"æ®µè½æ”¹è¿›æ”¶æ•› ({prev_avg:.1f} â†’ {curr_avg:.1f})ï¼Œè·³è¿‡")
            return "continue"

        state["prev_section_avg_score"] = curr_avg
        return "improve"

    def _section_improve_node(self, state: SharedState) -> SharedState:
        """æ®µè½ç²¾å‡†æ”¹è¿›èŠ‚ç‚¹ï¼ˆGenerator è§’è‰²ï¼‰"""
        logger.info("=== Step 4.3: æ®µè½ç²¾å‡†æ”¹è¿› ===")
        evaluations = state.get("section_evaluations", [])
        sections = state.get("sections", [])
        improved_count = 0

        for evaluation in evaluations:
            idx = evaluation.get("section_idx", -1)
            if evaluation["overall_quality"] >= 7.0 or idx < 0 or idx >= len(sections):
                continue

            section = sections[idx]
            improved_content = self.writer.improve_section(
                original_content=section.get("content", ""),
                critique=evaluation,
                section_title=section.get("title", ""),
            )
            section["content"] = improved_content
            improved_count += 1

        state["section_improve_count"] = state.get("section_improve_count", 0) + 1
        logger.info(f"æ®µè½æ”¹è¿›å®Œæˆ: æ”¹è¿›äº† {improved_count} ä¸ªæ®µè½ (ç¬¬ {state['section_improve_count']} è½®)")

        # 69.05: è®°å½•æ®µè½æ”¹è¿›å¿«ç…§
        new_avg = (
            sum(e["overall_quality"] for e in evaluations) / max(len(evaluations), 1)
        )
        self.tracker.log_section_improve_snapshot(
            round_num=state["section_improve_count"],
            improved_count=improved_count,
            avg_score_before=state.get("prev_section_avg_score", 0),
            avg_score_after=new_avg,
        )

        return state
    
    def _coder_and_artist_node(self, state: SharedState) -> SharedState:
        """ä»£ç å’Œé…å›¾å¹¶è¡Œç”ŸæˆèŠ‚ç‚¹"""
        from concurrent.futures import ThreadPoolExecutor
        
        logger.info("=== Step 5: ä»£ç å’Œé…å›¾å¹¶è¡Œç”Ÿæˆ ===")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ coder å’Œ artist
        # coder ä¿®æ”¹: code_blocks, sections[x].code_ids
        # artist ä¿®æ”¹: images, sections[x].image_ids
        # ä¸¤è€…ä¸å†²çªï¼Œå¯ä»¥å®‰å…¨å¹¶è¡Œ
        
        def run_coder():
            logger.info("â†’ å¼€å§‹ä»£ç ç”Ÿæˆ")
            return self.coder.run(state)
        
        def run_artist():
            logger.info("â†’ å¼€å§‹é…å›¾ç”Ÿæˆï¼ˆå«è¡¥å›¾æ£€æµ‹ï¼‰")
            return self.artist.run(state)
        
        with ThreadPoolExecutor(max_workers=2) as executor:
            coder_future = executor.submit(run_coder)
            artist_future = executor.submit(run_artist)
            
            # ç­‰å¾…ä¸¤è€…å®Œæˆï¼Œå¹¶åˆå¹¶ç»“æœ
            coder_result = coder_future.result()
            artist_result = artist_future.result()
            
            # åˆå¹¶ artist çš„ç»“æœåˆ° stateï¼ˆç‰¹åˆ«æ˜¯ section_imagesï¼‰
            if artist_result:
                if 'section_images' in artist_result:
                    state['section_images'] = artist_result['section_images']
                    logger.info(f"åˆå¹¶ section_images: {len(state['section_images'])} å¼ ")
                if 'images' in artist_result:
                    state['images'] = artist_result['images']
        
        code_count = len(state.get('code_blocks', []))
        image_count = len(state.get('images', []))
        logger.info(f"=== ä»£ç å’Œé…å›¾å¹¶è¡Œç”Ÿæˆå®Œæˆ: {code_count} ä¸ªä»£ç å—, {image_count} å¼ å›¾ç‰‡ ===")

        # 69.05: è®°å½•é…å›¾ç”Ÿæˆç»“æœ
        for img in state.get('images', []):
            self.tracker.log_image_generation(
                image_id=img.get('id', ''),
                image_type=img.get('render_method', ''),
                success=True,
            )

        return state
    
    def _reviewer_node(self, state: SharedState) -> SharedState:
        """è´¨é‡å®¡æ ¸èŠ‚ç‚¹"""
        logger.info("=== Step 7: è´¨é‡å®¡æ ¸ ===")
        state = self.reviewer.run(state)

        # åˆå¹¶ä¸€è‡´æ€§æ£€æŸ¥å‘ç°çš„é—®é¢˜åˆ° review_issues
        consistency_issues = state.get('thread_issues', []) + state.get('voice_issues', [])
        if consistency_issues:
            existing = state.get('review_issues', [])
            state['review_issues'] = existing + consistency_issues
            logger.info(f"[Reviewer] åˆå¹¶ä¸€è‡´æ€§æ£€æŸ¥é—®é¢˜: {len(consistency_issues)} æ¡")

        # 69.05: è®°å½•å®¡æ ¸åˆ†æ•°åˆ° Langfuse
        self.tracker.log_review_score(
            score=state.get('review_score', 0),
            round_num=state.get('revision_count', 0),
            summary=f"issues={len(state.get('review_issues', []))} approved={state.get('review_approved', False)}",
        )

        return state
    
    def _revision_node(self, state: SharedState) -> SharedState:
        """ä¿®è®¢èŠ‚ç‚¹ï¼ˆå¹¶è¡Œï¼‰"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import os
        
        logger.info("=== Step 7.1: ä¿®è®¢ ===")
        before_count = _get_content_word_count(state)
        state['revision_count'] = state.get('revision_count', 0) + 1
        
        # æ ¹æ®å®¡æ ¸é—®é¢˜ä¿®è®¢å†…å®¹
        review_issues = state.get('review_issues', [])
        total_issues = len(review_issues)
        style = self._get_style(state)

        if total_issues == 0:
            logger.info("æ²¡æœ‰éœ€è¦ä¿®è®¢çš„é—®é¢˜")
            return state
        
        max_workers = int(os.environ.get('BLOG_GENERATOR_MAX_WORKERS', '3'))
        use_parallel = _should_use_parallel()
        
        # correct_only æ¨¡å¼ï¼šæŒ‰ç« èŠ‚åˆ†ç»„é—®é¢˜ï¼Œä½¿ç”¨ correct_sectionï¼ˆåªæ›´æ­£ä¸æ‰©å±•ï¼‰
        if style.revision_strategy == "correct_only":
            # æŒ‰ç« èŠ‚åˆ†ç»„é—®é¢˜
            section_issues = {}
            for issue in review_issues:
                section_id = issue.get('section_id', '')
                if section_id not in section_issues:
                    section_issues[section_id] = []
                section_issues[section_id].append({
                    'severity': issue.get('severity', 'medium'),
                    'description': issue.get('description', ''),
                    'affected_content': issue.get('affected_content', '')
                })
            
            if use_parallel:
                logger.info(f"å¼€å§‹æ›´æ­£ {len(section_issues)} ä¸ªç« èŠ‚ï¼Œä½¿ç”¨ {min(max_workers, len(section_issues))} ä¸ªå¹¶è¡Œçº¿ç¨‹")
            else:
                logger.info(f"å¼€å§‹æ›´æ­£ {len(section_issues)} ä¸ªç« èŠ‚ï¼Œä½¿ç”¨ä¸²è¡Œæ¨¡å¼ï¼ˆè¿½è¸ªå·²å¯ç”¨ï¼‰")
            
            # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
            tasks = []
            for idx, (section_id, issues) in enumerate(section_issues.items(), 1):
                for section in state.get('sections', []):
                    if section.get('id') == section_id:
                        tasks.append({
                            'section': section,
                            'section_id': section_id,
                            'issues': issues,
                            'progress_info': f"[{idx}/{len(section_issues)}]"
                        })
                        break
            
            # æ‰§è¡Œæ›´æ­£
            def correct_single_task(task):
                try:
                    section = task['section']
                    section_title = section.get('title', task['section_id'])
                    corrected_content = self.writer.correct_section(
                        original_content=section.get('content', ''),
                        issues=task['issues'],
                        section_title=section_title,
                        progress_info=task['progress_info']
                    )
                    return {
                        'success': True,
                        'section_id': task['section_id'],
                        'content': corrected_content
                    }
                except Exception as e:
                    logger.error(f"ç« èŠ‚æ›´æ­£å¤±è´¥ [{task['section_id']}]: {e}")
                    return {
                        'success': False,
                        'section_id': task['section_id'],
                        'content': None
                    }
            
            if use_parallel:
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = {executor.submit(correct_single_task, task): task for task in tasks}
                    for future in as_completed(futures):
                        result = future.result()
                        if result['success']:
                            for section in state.get('sections', []):
                                if section.get('id') == result['section_id']:
                                    section['content'] = result['content']
                                    break
            else:
                # ä¸²è¡Œæ‰§è¡Œï¼ˆè¿½è¸ªæ¨¡å¼ï¼‰- ç›´æ¥è°ƒç”¨æ–¹æ³•ä»¥ä¿æŒ Langfuse ä¸Šä¸‹æ–‡
                for task in tasks:
                    try:
                        section = task['section']
                        section_title = section.get('title', task['section_id'])
                        corrected_content = self.writer.correct_section(
                            original_content=section.get('content', ''),
                            issues=task['issues'],
                            section_title=section_title,
                            progress_info=task['progress_info']
                        )
                        for s in state.get('sections', []):
                            if s.get('id') == task['section_id']:
                                s['content'] = corrected_content
                                break
                    except Exception as e:
                        logger.error(f"ç« èŠ‚æ›´æ­£å¤±è´¥ [{task['section_id']}]: {e}")
        else:
            # å…¶ä»–æ¨¡å¼ï¼šä½¿ç”¨ enhance_sectionï¼ˆå¯æ‰©å±•å†…å®¹ï¼‰
            if use_parallel:
                logger.info(f"å¼€å§‹ä¿®è®¢ {total_issues} ä¸ªé—®é¢˜ï¼Œä½¿ç”¨ {min(max_workers, total_issues)} ä¸ªå¹¶è¡Œçº¿ç¨‹")
            else:
                logger.info(f"å¼€å§‹ä¿®è®¢ {total_issues} ä¸ªé—®é¢˜ï¼Œä½¿ç”¨ä¸²è¡Œæ¨¡å¼ï¼ˆè¿½è¸ªå·²å¯ç”¨ï¼‰")
            
            # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
            tasks = []
            for idx, issue in enumerate(review_issues, 1):
                section_id = issue.get('section_id', '')
                suggestion = issue.get('suggestion', '')
                
                for section in state.get('sections', []):
                    if section.get('id') == section_id:
                        tasks.append({
                            'section': section,
                            'section_id': section_id,
                            'issue': issue,
                            'suggestion': suggestion,
                            'progress_info': f"[{idx}/{total_issues}]"
                        })
                        break
            
            # æ‰§è¡Œå¢å¼º
            def enhance_single_task(task):
                try:
                    section = task['section']
                    section_title = section.get('title', task['section_id'])
                    enhanced_content = self.writer.enhance_section(
                        original_content=section.get('content', ''),
                        vague_points=[{
                            'location': section_title,
                            'issue': task['issue'].get('description', ''),
                            'question': task['suggestion'],
                            'suggestion': 'æ ¹æ®å®¡æ ¸å»ºè®®ä¿®æ”¹'
                        }],
                        section_title=section_title,
                        progress_info=task['progress_info']
                    )
                    return {
                        'success': True,
                        'section_id': task['section_id'],
                        'content': enhanced_content
                    }
                except Exception as e:
                    logger.error(f"ç« èŠ‚ä¿®è®¢å¤±è´¥ [{task['section_id']}]: {e}")
                    return {
                        'success': False,
                        'section_id': task['section_id'],
                        'content': None
                    }
            
            if use_parallel:
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = {executor.submit(enhance_single_task, task): task for task in tasks}
                    for future in as_completed(futures):
                        result = future.result()
                        if result['success']:
                            for section in state.get('sections', []):
                                if section.get('id') == result['section_id']:
                                    section['content'] = result['content']
                                    break
            else:
                # ä¸²è¡Œæ‰§è¡Œï¼ˆè¿½è¸ªæ¨¡å¼ï¼‰- ç›´æ¥è°ƒç”¨æ–¹æ³•ä»¥ä¿æŒ Langfuse ä¸Šä¸‹æ–‡
                for task in tasks:
                    try:
                        section = task['section']
                        section_title = section.get('title', task['section_id'])
                        enhanced_content = self.writer.enhance_section(
                            original_content=section.get('content', ''),
                            vague_points=[{
                                'location': section_title,
                                'issue': task['issue'].get('description', ''),
                                'question': task['suggestion'],
                                'suggestion': 'æ ¹æ®å®¡æ ¸å»ºè®®ä¿®æ”¹'
                            }],
                            section_title=section_title,
                            progress_info=task['progress_info']
                        )
                        for s in state.get('sections', []):
                            if s.get('id') == task['section_id']:
                                s['content'] = enhanced_content
                                break
                    except Exception as e:
                        logger.error(f"ç« èŠ‚ä¿®è®¢å¤±è´¥ [{task['section_id']}]: {e}")
        
        after_count = _get_content_word_count(state)
        _log_word_count_diff("ä¿®è®¢", before_count, after_count)
        return state

    def _consistency_check_node(self, state: SharedState) -> SharedState:
        """ä¸€è‡´æ€§æ£€æŸ¥èŠ‚ç‚¹ï¼ˆThread + Voice å¹¶è¡Œï¼‰"""
        from concurrent.futures import ThreadPoolExecutor

        sections = state.get('sections', [])
        if len(sections) < 2:
            state['thread_issues'] = []
            state['voice_issues'] = []
            return state

        style = self._get_style(state)
        thread_enabled = self._is_enabled(self._env_thread_check, style.enable_thread_check)
        voice_enabled = self._is_enabled(self._env_voice_check, style.enable_voice_check)

        if not thread_enabled and not voice_enabled:
            state['thread_issues'] = []
            state['voice_issues'] = []
            return state

        logger.info("=== Step 6.5: ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆå™äº‹ + è¯­æ°”ï¼‰===")

        use_parallel = _should_use_parallel()

        if use_parallel and thread_enabled and voice_enabled:
            with ThreadPoolExecutor(max_workers=2) as executor:
                futures = []
                if thread_enabled:
                    futures.append(executor.submit(self.thread_checker.run, state))
                if voice_enabled:
                    futures.append(executor.submit(self.voice_checker.run, state))
                for f in futures:
                    f.result()
        else:
            if thread_enabled:
                try:
                    self.thread_checker.run(state)
                except Exception as e:
                    logger.error(f"[ThreadChecker] å¼‚å¸¸: {e}")
                    state['thread_issues'] = []
            if voice_enabled:
                try:
                    self.voice_checker.run(state)
                except Exception as e:
                    logger.error(f"[VoiceChecker] å¼‚å¸¸: {e}")
                    state['voice_issues'] = []

        if not thread_enabled:
            state['thread_issues'] = []
        if not voice_enabled:
            state['voice_issues'] = []

        thread_count = len(state.get('thread_issues', []))
        voice_count = len(state.get('voice_issues', []))
        logger.info(f"[ConsistencyCheck] å®Œæˆ: å™äº‹é—®é¢˜ {thread_count}, è¯­æ°”é—®é¢˜ {voice_count}")
        return state

    def _get_style(self, state: SharedState) -> StyleProfile:
        """è·å–å½“å‰è¿è¡Œçš„ StyleProfileï¼ˆå®ä¾‹çº§ > state çº§ > target_length æ¨æ–­ï¼‰"""
        if self.style:
            return self.style
        target_length = state.get('target_length', 'medium')
        return StyleProfile.from_target_length(target_length)

    def _is_enabled(self, env_flag: bool, style_flag: bool) -> bool:
        """ç¯å¢ƒå˜é‡ AND StyleProfile åŒé‡å¼€å…³"""
        return env_flag and style_flag

    def _factcheck_node(self, state: SharedState) -> SharedState:
        """äº‹å®æ ¸æŸ¥èŠ‚ç‚¹"""
        style = self._get_style(state)
        if not self._is_enabled(self._env_factcheck, style.enable_fact_check):
            logger.info("=== Step 7.3: äº‹å®æ ¸æŸ¥ï¼ˆå·²ç¦ç”¨ï¼Œè·³è¿‡ï¼‰===")
            return state
        logger.info("=== Step 7.3: äº‹å®æ ¸æŸ¥ ===")
        try:
            return self.factcheck.run(state)
        except Exception as e:
            logger.error(f"[FactCheck] å¼‚å¸¸ï¼Œé™çº§è·³è¿‡: {e}")
            return state

    def _text_cleanup_node(self, state: SharedState) -> SharedState:
        """ç¡®å®šæ€§æ–‡æœ¬æ¸…ç†èŠ‚ç‚¹ï¼ˆçº¯æ­£åˆ™ï¼Œé›¶ LLMï¼‰"""
        style = self._get_style(state)
        if not self._is_enabled(self._env_text_cleanup, style.enable_text_cleanup):
            logger.info("=== Step 7.4: æ–‡æœ¬æ¸…ç†ï¼ˆå·²ç¦ç”¨ï¼Œè·³è¿‡ï¼‰===")
            return state
        logger.info("=== Step 7.4: ç¡®å®šæ€§æ–‡æœ¬æ¸…ç† ===")
        from utils.text_cleanup import apply_full_cleanup
        total_fixes = 0
        for section in state.get('sections', []):
            content = section.get('content', '')
            if not content:
                continue
            result = apply_full_cleanup(content)
            section['content'] = result['text']
            fixes = result['total_fixes']
            if fixes:
                logger.info(f"  [{section.get('title', '')}] ä¿®å¤ {fixes} å¤„: {result['stats']}")
                total_fixes += fixes
        logger.info(f"[TextCleanup] å®Œæˆ: å…±ä¿®å¤ {total_fixes} å¤„")
        return state

    def _humanizer_node(self, state: SharedState) -> SharedState:
        """å» AI å‘³èŠ‚ç‚¹"""
        style = self._get_style(state)
        if not self._is_enabled(self._env_humanizer, style.enable_humanizer):
            logger.info("=== Step 7.5: å» AI å‘³ï¼ˆå·²ç¦ç”¨ï¼Œè·³è¿‡ï¼‰===")
            return state
        logger.info("=== Step 7.5: å» AI å‘³ ===")
        try:
            return self.humanizer.run(state)
        except Exception as e:
            logger.error(f"[Humanizer] å¼‚å¸¸ï¼Œé™çº§ä½¿ç”¨åŸå§‹å†…å®¹: {e}")
            return state

    def _summary_generator_node(self, state: SharedState) -> SharedState:
        """åšå®¢å¯¼è¯» + SEO å…³é”®è¯ç”ŸæˆèŠ‚ç‚¹"""
        style = self._get_style(state)
        if not self._is_enabled(self._env_summary, style.enable_summary_gen):
            logger.info("=== Step 9: å¯¼è¯»+SEOï¼ˆå·²ç¦ç”¨ï¼Œè·³è¿‡ï¼‰===")
            return state
        logger.info("=== Step 9: å¯¼è¯» + SEO å…³é”®è¯ç”Ÿæˆ ===")
        try:
            return self.summary_generator.run(state)
        except Exception as e:
            logger.error(f"[SummaryGenerator] å¼‚å¸¸ï¼Œé™çº§è·³è¿‡: {e}")
            return state

    def _assembler_node(self, state: SharedState) -> SharedState:
        """æ–‡æ¡£ç»„è£…èŠ‚ç‚¹"""
        logger.info("=== Step 8: æ–‡æ¡£ç»„è£… ===")
        return self.assembler.run(state)
    
    def _should_deepen(self, state: SharedState) -> Literal["deepen", "continue"]:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ·±åŒ–å†…å®¹"""
        if not state.get('all_sections_detailed', True):
            if state.get('questioning_count', 0) < self.max_questioning_rounds:
                return "deepen"
        return "continue"
    
    def _should_revise(self, state: SharedState) -> Literal["revision", "assemble"]:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿®è®¢ â€” ç”± StyleProfile æ§åˆ¶"""
        style = self._get_style(state)
        revision_count = state.get('revision_count', 0)

        # è¾¾åˆ°æœ€å¤§ä¿®è®¢è½®æ•°
        if revision_count >= style.max_revision_rounds:
            logger.info(f"å·²è¾¾åˆ°æœ€å¤§ä¿®è®¢è½®æ•° ({style.max_revision_rounds})ï¼Œè·³è¿‡ä¿®è®¢")
            return "assemble"

        review_issues = state.get('review_issues', [])

        # ä¿®è®¢é—®é¢˜è¿‡æ»¤ï¼ˆhigh_only æ¨¡å¼ï¼‰
        if style.revision_severity_filter == "high_only":
            high_issues = [i for i in review_issues if i.get('severity') == 'high']
            if high_issues:
                logger.info(f"[{style.revision_severity_filter}] åªå¤„ç† {len(high_issues)} ä¸ª high çº§åˆ«é—®é¢˜")
                state['review_issues'] = high_issues
                return "revision"
            logger.info(f"[{style.revision_severity_filter}] æ—  high çº§åˆ«é—®é¢˜ï¼Œè·³è¿‡ä¿®è®¢")
            return "assemble"

        # å®Œæ•´ä¿®è®¢æ¨¡å¼
        if not state.get('review_approved', True):
            return "revision"

        logger.info("å®¡æ ¸é€šè¿‡æˆ–ä¿®è®¢å®Œæˆï¼Œè¿›å…¥ç»„è£…")
        return "assemble"

    def _should_refine_search(self, state: SharedState) -> Literal["search", "continue"]:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç»†åŒ–æœç´¢ â€” ç”± StyleProfile æ§åˆ¶"""
        style = self._get_style(state)
        if not style.enable_knowledge_refinement:
            logger.info("çŸ¥è¯†å¢å¼ºå·²ç¦ç”¨ï¼Œè·³è¿‡")
            return "continue"

        gaps = state.get('knowledge_gaps', [])
        search_count = state.get('search_count', 0)
        max_count = state.get('max_search_count', 5)

        if gaps and search_count < max_count:
            important_gaps = [g for g in gaps if g.get('gap_type') in ['missing_data', 'vague_concept']]
            if important_gaps:
                logger.info(f"æ£€æµ‹åˆ° {len(important_gaps)} ä¸ªé‡è¦çŸ¥è¯†ç©ºç™½ï¼Œè§¦å‘ç»†åŒ–æœç´¢")
                return "search"

        logger.info("æ— éœ€ç»†åŒ–æœç´¢ï¼Œç»§ç»­åˆ°è¿½é—®é˜¶æ®µ")
        return "continue"

    def _run_derivative_skills(self, final_state: Dict[str, Any]) -> Dict[str, Any]:
        """37.14/37.16 è¿è¡Œåšå®¢è¡ç”Ÿç‰© Skillsï¼ˆMindMap/Flashcard/StudyNoteï¼‰"""
        if os.environ.get('SKILL_DERIVATIVES_ENABLED', 'false').lower() != 'true':
            return {}
        try:
            from .skills.executor import SkillExecutor
            from .skills.registry import SkillRegistry
            # ç¡®ä¿ skills å·²æ³¨å†Œï¼ˆå¯¼å…¥è§¦å‘ @register è£…é¥°å™¨ï¼‰
            from .skills import mindmap, flashcard, study_note  # noqa: F401

            executor = SkillExecutor()
            markdown = final_state.get('final_markdown', '')
            if not markdown:
                return {}

            input_data = {"markdown": markdown, "topic": final_state.get('topic', '')}
            results = {}
            for skill_name in SkillRegistry.get_post_process_skills():
                try:
                    result = executor.execute(skill_name, input_data)
                    if result.get('success'):
                        results[skill_name] = result.get('output')
                        logger.info(f"ğŸ¯ è¡ç”Ÿç‰© [{skill_name}] ç”Ÿæˆå®Œæˆ")
                except Exception as e:
                    logger.warning(f"è¡ç”Ÿç‰© [{skill_name}] ç”Ÿæˆå¤±è´¥: {e}")
            return results
        except Exception as e:
            logger.warning(f"è¡ç”Ÿç‰©ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return {}

    def compile(self, checkpointer=None):
        """
        ç¼–è¯‘å·¥ä½œæµ
        
        Args:
            checkpointer: æ£€æŸ¥ç‚¹å­˜å‚¨ (å¯é€‰)
        """
        if checkpointer is None:
            checkpointer = MemorySaver()
        
        self.app = self.workflow.compile(checkpointer=checkpointer)
        return self.app
    
    def generate(
        self,
        topic: str,
        article_type: str = "tutorial",
        target_audience: str = "intermediate",
        target_length: str = "medium",
        source_material: str = None,
        on_progress: Callable[[str, str], None] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆåšå®¢
        
        Args:
            topic: æŠ€æœ¯ä¸»é¢˜
            article_type: æ–‡ç« ç±»å‹
            target_audience: ç›®æ ‡å—ä¼—
            target_length: ç›®æ ‡é•¿åº¦
            source_material: å‚è€ƒèµ„æ–™
            on_progress: è¿›åº¦å›è°ƒ
            
        Returns:
            ç”Ÿæˆç»“æœ
        """
        if self.app is None:
            self.compile()

        # åˆ›å»º Token è¿½è¸ªå™¨å¹¶æ³¨å…¥ LLMService
        token_tracker = None
        try:
            import os
            if os.environ.get('TOKEN_TRACKING_ENABLED', 'true').lower() == 'true':
                from utils.token_tracker import TokenTracker
                token_tracker = TokenTracker()
                self.llm.token_tracker = token_tracker
        except Exception:
            pass

        # åˆ›å»ºç»“æ„åŒ–ä»»åŠ¡æ—¥å¿—
        task_log = None
        try:
            import os as _os
            if _os.environ.get('BLOG_TASK_LOG_ENABLED', 'true').lower() == 'true':
                from .utils.task_log import BlogTaskLog
                task_log = BlogTaskLog(
                    topic=topic,
                    article_type=article_type,
                    target_length=target_length,
                )
                self.task_log = task_log
        except Exception:
            pass

        # åˆ›å»º ToolManager å¹¶æ³¨å†Œç°æœ‰å·¥å…·ï¼ˆ37.09ï¼‰
        try:
            from utils.tool_manager import BlogToolManager
            tool_manager = BlogToolManager(task_log=task_log)
            if self.search_service:
                tool_manager.register(
                    "web_search", self.search_service.search,
                    description="æœç´¢äº’è”ç½‘è·å–èƒŒæ™¯çŸ¥è¯†", timeout=30,
                )
            self.tool_manager = tool_manager
        except Exception:
            pass

        # åˆ›å»ºåˆå§‹çŠ¶æ€
        initial_state = create_initial_state(
            topic=topic,
            article_type=article_type,
            target_audience=target_audience,
            target_length=target_length,
            source_material=source_material
        )
        
        logger.info(f"å¼€å§‹ç”Ÿæˆåšå®¢: {topic}")
        logger.info(f"  ç±»å‹: {article_type}, å—ä¼—: {target_audience}, é•¿åº¦: {target_length}")
        
        # æ‰§è¡Œå·¥ä½œæµ
        config = {"configurable": {"thread_id": f"blog_{topic}"}}
        
        try:
            final_state = self.app.invoke(initial_state, config)
            
            logger.info("åšå®¢ç”Ÿæˆå®Œæˆ!")

            # è¾“å‡º Token ç”¨é‡æ‘˜è¦
            token_summary = None
            if token_tracker:
                logger.info(token_tracker.format_summary())
                token_summary = token_tracker.get_summary()

            # å®Œæˆä»»åŠ¡æ—¥å¿—
            if task_log:
                task_log.complete(
                    score=final_state.get('review_score', 0),
                    word_count=len(final_state.get('final_markdown', '')),
                    revision_rounds=final_state.get('revision_count', 0),
                )
                if token_summary:
                    task_log.token_summary = token_summary
                try:
                    task_log.save()
                except Exception as save_err:
                    logger.warning(f"ä»»åŠ¡æ—¥å¿—ä¿å­˜å¤±è´¥: {save_err}")
                logger.info(task_log.get_summary())

            result = {
                "success": True,
                "markdown": final_state.get('final_markdown', ''),
                "outline": final_state.get('outline', {}),
                "sections_count": len(final_state.get('sections', [])),
                "images_count": len(final_state.get('images', [])),
                "code_blocks_count": len(final_state.get('code_blocks', [])),
                "review_score": final_state.get('review_score', 0),
                "seo_keywords": final_state.get('seo_keywords', []),
                "social_summary": final_state.get('social_summary', ''),
                "meta_description": final_state.get('meta_description', ''),
                "error": None
            }
            if token_summary:
                result["token_summary"] = token_summary

            # 37.14/37.16 åšå®¢è¡ç”Ÿç‰©ç”Ÿæˆï¼ˆSkill åå¤„ç†ï¼‰
            derivatives = self._run_derivative_skills(final_state)
            if derivatives:
                result["derivatives"] = derivatives

            return result
            
        except Exception as e:
            logger.error(f"åšå®¢ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            if task_log:
                task_log.fail(str(e))
                try:
                    task_log.save()
                except Exception:
                    pass
            return {
                "success": False,
                "markdown": "",
                "error": str(e)
            }
    
    async def generate_stream(
        self,
        topic: str,
        article_type: str = "tutorial",
        target_audience: str = "intermediate",
        target_length: str = "medium",
        source_material: str = None
    ):
        """
        æµå¼ç”Ÿæˆåšå®¢ (å¼‚æ­¥ç”Ÿæˆå™¨)
        
        Args:
            topic: æŠ€æœ¯ä¸»é¢˜
            article_type: æ–‡ç« ç±»å‹
            target_audience: ç›®æ ‡å—ä¼—
            target_length: ç›®æ ‡é•¿åº¦
            source_material: å‚è€ƒèµ„æ–™
            
        Yields:
            ç”Ÿæˆè¿›åº¦å’Œä¸­é—´ç»“æœ
        """
        if self.app is None:
            self.compile()
        
        initial_state = create_initial_state(
            topic=topic,
            article_type=article_type,
            target_audience=target_audience,
            target_length=target_length,
            source_material=source_material
        )
        
        config = {"configurable": {"thread_id": f"blog_{topic}"}}
        
        # ä½¿ç”¨ stream æ–¹æ³•è·å–ä¸­é—´çŠ¶æ€
        for event in self.app.stream(initial_state, config):
            for node_name, state in event.items():
                yield {
                    "stage": node_name,
                    "state": state
                }
