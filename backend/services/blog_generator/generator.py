"""
é•¿æ–‡åšå®¢ç”Ÿæˆå™¨ - LangGraph å·¥ä½œæµä¸»å…¥å£
"""

import logging
from typing import Dict, Any, Optional, Literal, Callable

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

from .schemas.state import SharedState, create_initial_state
from .agents.researcher import ResearcherAgent
from .agents.planner import PlannerAgent
from .agents.writer import WriterAgent
from .agents.coder import CoderAgent
from .agents.artist import ArtistAgent
from .agents.questioner import QuestionerAgent
from .agents.reviewer import ReviewerAgent
from .agents.assembler import AssemblerAgent
from .agents.search_coordinator import SearchCoordinator

logger = logging.getLogger(__name__)


def _get_content_word_count(state: Dict[str, Any]) -> int:
    """è®¡ç®—å½“å‰ state ä¸­æ‰€æœ‰ç« èŠ‚å†…å®¹çš„æ€»å­—æ•°"""
    sections = state.get('sections', [])
    total = 0
    for section in sections:
        content = section.get('content', '')
        if content:
            total += len(content)
    return total


def _log_word_count_diff(agent_name: str, before: int, after: int):
    """è®°å½•å­—æ•°å˜åŒ–çš„ diff"""
    diff = after - before
    if diff >= 0:
        logger.info(f"ğŸ“Š [{agent_name}] å­—æ•°å˜åŒ–: {before} â†’ {after} (+{diff} å­—)")
    else:
        logger.info(f"ğŸ“Š [{agent_name}] å­—æ•°å˜åŒ–: {before} â†’ {after} ({diff} å­—)")


class BlogGenerator:
    """
    é•¿æ–‡åšå®¢ç”Ÿæˆå™¨
    
    åŸºäº LangGraph å®ç°çš„ Multi-Agent ååŒç”Ÿæˆç³»ç»Ÿ
    """
    
    def __init__(
        self,
        llm_client,
        search_service=None,
        knowledge_service=None,
        max_questioning_rounds: int = 2,
        max_revision_rounds: int = 3
    ):
        """
        åˆå§‹åŒ–åšå®¢ç”Ÿæˆå™¨
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            search_service: æœç´¢æœåŠ¡ (å¯é€‰)
            knowledge_service: çŸ¥è¯†æœåŠ¡ (å¯é€‰ï¼Œç”¨äºæ–‡æ¡£çŸ¥è¯†èåˆ)
            max_questioning_rounds: æœ€å¤§è¿½é—®è½®æ•°
            max_revision_rounds: æœ€å¤§ä¿®è®¢è½®æ•°
        """
        self.llm = llm_client
        self.search_service = search_service
        self.knowledge_service = knowledge_service
        self.max_questioning_rounds = max_questioning_rounds
        self.max_revision_rounds = max_revision_rounds
        
        # åˆå§‹åŒ–å„ Agent
        self.researcher = ResearcherAgent(llm_client, search_service, knowledge_service)
        self.planner = PlannerAgent(llm_client)
        self.writer = WriterAgent(llm_client)
        self.coder = CoderAgent(llm_client)
        self.artist = ArtistAgent(llm_client)
        self.questioner = QuestionerAgent(llm_client)
        self.reviewer = ReviewerAgent(llm_client)
        self.assembler = AssemblerAgent()
        self.search_coordinator = SearchCoordinator(llm_client, search_service)
        
        # æ„å»ºå·¥ä½œæµ
        self.workflow = self._build_workflow()
        self.app = None
    
    def _build_workflow(self) -> StateGraph:
        """
        æ„å»º LangGraph å·¥ä½œæµ
        
        Returns:
            StateGraph å®ä¾‹
        """
        workflow = StateGraph(SharedState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("researcher", self._researcher_node)
        workflow.add_node("planner", self._planner_node)
        workflow.add_node("writer", self._writer_node)
        # å¤šè½®æœç´¢ç›¸å…³èŠ‚ç‚¹
        workflow.add_node("check_knowledge", self._check_knowledge_node)
        workflow.add_node("refine_search", self._refine_search_node)
        workflow.add_node("enhance_with_knowledge", self._enhance_with_knowledge_node)
        # è¿½é—®å’Œå®¡æ ¸èŠ‚ç‚¹
        workflow.add_node("questioner", self._questioner_node)
        workflow.add_node("deepen_content", self._deepen_content_node)
        workflow.add_node("coder_and_artist", self._coder_and_artist_node)  # å¹¶è¡ŒèŠ‚ç‚¹
        workflow.add_node("reviewer", self._reviewer_node)
        workflow.add_node("revision", self._revision_node)
        workflow.add_node("assembler", self._assembler_node)
        
        # å®šä¹‰è¾¹
        workflow.add_edge(START, "researcher")
        workflow.add_edge("researcher", "planner")
        workflow.add_edge("planner", "writer")
        
        # Writer åè¿›å…¥çŸ¥è¯†ç©ºç™½æ£€æŸ¥
        workflow.add_edge("writer", "check_knowledge")
        
        # æ¡ä»¶è¾¹ï¼šæ£€æŸ¥åå†³å®šæ˜¯æœç´¢è¿˜æ˜¯ç»§ç»­åˆ° Questioner
        workflow.add_conditional_edges(
            "check_knowledge",
            self._should_refine_search,
            {
                "search": "refine_search",
                "continue": "questioner"
            }
        )
        
        # æœç´¢åå¢å¼ºå†…å®¹ï¼Œç„¶åå›åˆ°çŸ¥è¯†æ£€æŸ¥
        workflow.add_edge("refine_search", "enhance_with_knowledge")
        workflow.add_edge("enhance_with_knowledge", "check_knowledge")
        
        # æ¡ä»¶è¾¹ï¼šè¿½é—®åå†³å®šæ˜¯æ·±åŒ–è¿˜æ˜¯ç»§ç»­
        workflow.add_conditional_edges(
            "questioner",
            self._should_deepen,
            {
                "deepen": "deepen_content",
                "continue": "coder_and_artist"  # è¿›å…¥å¹¶è¡ŒèŠ‚ç‚¹
            }
        )
        workflow.add_edge("deepen_content", "questioner")  # æ·±åŒ–åé‡æ–°è¿½é—®
        
        # Coder å’Œ Artist å¹¶è¡Œæ‰§è¡Œï¼ˆé€šè¿‡å•ä¸ªèŠ‚ç‚¹å†…éƒ¨å¹¶è¡Œå®ç°ï¼‰
        workflow.add_edge("coder_and_artist", "reviewer")
        
        # æ¡ä»¶è¾¹ï¼šå®¡æ ¸åå†³å®šæ˜¯ä¿®è®¢è¿˜æ˜¯ç»„è£…
        workflow.add_conditional_edges(
            "reviewer",
            self._should_revise,
            {
                "revision": "revision",
                "assemble": "assembler"
            }
        )
        workflow.add_edge("revision", "reviewer")  # ä¿®è®¢åé‡æ–°å®¡æ ¸
        workflow.add_edge("assembler", END)
        
        return workflow
    
    def _researcher_node(self, state: SharedState) -> SharedState:
        """ç´ ææ”¶é›†èŠ‚ç‚¹"""
        logger.info("=== Step 1: ç´ ææ”¶é›† ===")
        return self.researcher.run(state)
    
    def _planner_node(self, state: SharedState) -> SharedState:
        """å¤§çº²è§„åˆ’èŠ‚ç‚¹"""
        logger.info("=== Step 2: å¤§çº²è§„åˆ’ ===")
        # ä½¿ç”¨å®ä¾‹å˜é‡ä¸­çš„æµå¼å›è°ƒ
        on_stream = getattr(self, '_outline_stream_callback', None)
        return self.planner.run(state, on_stream=on_stream)
    
    def _writer_node(self, state: SharedState) -> SharedState:
        """å†…å®¹æ’°å†™èŠ‚ç‚¹"""
        logger.info("=== Step 3: å†…å®¹æ’°å†™ ===")
        before_count = _get_content_word_count(state)
        result = self.writer.run(state)
        after_count = _get_content_word_count(result)
        _log_word_count_diff("Writer", before_count, after_count)
        # åˆå§‹åŒ–ç´¯ç§¯çŸ¥è¯†ï¼ˆé¦–æ¬¡å†™ä½œåï¼‰
        if not result.get('accumulated_knowledge'):
            result['accumulated_knowledge'] = result.get('background_knowledge', '')
        return result
    
    def _check_knowledge_node(self, state: SharedState) -> SharedState:
        """çŸ¥è¯†ç©ºç™½æ£€æŸ¥èŠ‚ç‚¹"""
        search_count = state.get('search_count', 0)
        max_count = state.get('max_search_count', 5)
        logger.info(f"=== Step 3.5: çŸ¥è¯†ç©ºç™½æ£€æŸ¥ (æœç´¢æ¬¡æ•°: {search_count}/{max_count}) ===")
        return self.search_coordinator.run(state)
    
    def _refine_search_node(self, state: SharedState) -> SharedState:
        """ç»†åŒ–æœç´¢èŠ‚ç‚¹"""
        search_count = state.get('search_count', 0) + 1
        max_count = state.get('max_search_count', 5)
        logger.info(f"=== Step 3.6: ç»†åŒ–æœç´¢ (ç¬¬ {search_count} è½®) ===")
        
        gaps = state.get('knowledge_gaps', [])
        result = self.search_coordinator.refine_search(gaps, state)
        
        if result.get('success'):
            logger.info(f"ç»†åŒ–æœç´¢å®Œæˆ: è·å– {len(result.get('results', []))} æ¡ç»“æœ")
        else:
            logger.warning(f"ç»†åŒ–æœç´¢å¤±è´¥: {result.get('reason', 'æœªçŸ¥åŸå› ')}")
        
        return state
    
    def _enhance_with_knowledge_node(self, state: SharedState) -> SharedState:
        """åŸºäºæ–°çŸ¥è¯†å¢å¼ºå†…å®¹èŠ‚ç‚¹ï¼ˆå¹¶è¡Œï¼‰"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import os
        
        logger.info("=== Step 3.7: çŸ¥è¯†å¢å¼º ===")
        
        sections = state.get('sections', [])
        gaps = state.get('knowledge_gaps', [])
        new_knowledge = state.get('accumulated_knowledge', '')
        
        if not gaps or not new_knowledge:
            logger.info("æ²¡æœ‰éœ€è¦å¢å¼ºçš„å†…å®¹")
            return state
        
        from .prompts import get_prompt_manager
        pm = get_prompt_manager()
        
        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†éœ€è¦å¢å¼ºçš„ä»»åŠ¡
        tasks = []
        for section in sections:
            section_gaps = [g for g in gaps if not g.get('section_id') or g.get('section_id') == section.get('id')]
            
            if section_gaps:
                tasks.append({
                    'section': section,
                    'section_gaps': section_gaps,
                    'new_knowledge': new_knowledge
                })
        
        if not tasks:
            logger.info("æ²¡æœ‰éœ€è¦å¢å¼ºçš„ç« èŠ‚")
            state['knowledge_gaps'] = []
            return state
        
        max_workers = int(os.environ.get('BLOG_GENERATOR_MAX_WORKERS', '3'))
        logger.info(f"å¼€å§‹çŸ¥è¯†å¢å¼º: {len(tasks)} ä¸ªç« èŠ‚ï¼Œä½¿ç”¨ {min(max_workers, len(tasks))} ä¸ªå¹¶è¡Œçº¿ç¨‹")
        
        # ç¬¬äºŒæ­¥ï¼šå¹¶è¡Œå¢å¼º
        def enhance_task(task):
            """å•ä¸ªç« èŠ‚å¢å¼ºä»»åŠ¡"""
            section = task['section']
            try:
                prompt = pm.render_writer_enhance_with_knowledge(
                    original_content=section.get('content', ''),
                    new_knowledge=task['new_knowledge'],
                    knowledge_gaps=task['section_gaps']
                )
                
                enhanced_content = self.writer.llm.chat(
                    messages=[{"role": "user", "content": prompt}]
                )
                return {
                    'success': True,
                    'section': section,
                    'enhanced_content': enhanced_content
                }
            except Exception as e:
                logger.error(f"ç« èŠ‚å¢å¼ºå¤±è´¥ [{section.get('title', '')}]: {e}")
                return {
                    'success': False,
                    'section': section,
                    'error': str(e)
                }
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(enhance_task, task): task for task in tasks}
            
            for future in as_completed(futures):
                result = future.result()
                if result['success']:
                    result['section']['content'] = result['enhanced_content']
                    logger.info(f"ç« èŠ‚å¢å¼ºå®Œæˆ: {result['section'].get('title', '')}")
        
        enhanced_count = sum(1 for t in tasks if t['section'].get('content'))
        logger.info(f"çŸ¥è¯†å¢å¼ºå®Œæˆ: {enhanced_count} ä¸ªç« èŠ‚")
        
        # æ¸…ç©ºå·²å¤„ç†çš„çŸ¥è¯†ç©ºç™½
        state['knowledge_gaps'] = []
        
        return state
    
    
    
    def _questioner_node(self, state: SharedState) -> SharedState:
        """è¿½é—®æ£€æŸ¥èŠ‚ç‚¹"""
        logger.info("=== Step 4: è¿½é—®æ£€æŸ¥ ===")
        return self.questioner.run(state)
    
    def _deepen_content_node(self, state: SharedState) -> SharedState:
        """å†…å®¹æ·±åŒ–èŠ‚ç‚¹"""
        logger.info("=== Step 4.1: å†…å®¹æ·±åŒ– ===")
        before_count = _get_content_word_count(state)
        state['questioning_count'] = state.get('questioning_count', 0) + 1
        
        # ç»Ÿè®¡éœ€è¦æ·±åŒ–çš„ç« èŠ‚
        sections_to_deepen = [
            r for r in state.get('question_results', [])
            if not r.get('is_detailed_enough', True)
        ]
        total_to_deepen = len(sections_to_deepen)
        logger.info(f"å¼€å§‹æ·±åŒ– {total_to_deepen} ä¸ªç« èŠ‚")
        
        # æ ¹æ®è¿½é—®ç»“æœæ·±åŒ–å†…å®¹
        for idx, result in enumerate(sections_to_deepen, 1):
            section_id = result.get('section_id', '')
            vague_points = result.get('vague_points', [])
            
            # æ‰¾åˆ°å¯¹åº”ç« èŠ‚
            for section in state.get('sections', []):
                if section.get('id') == section_id:
                    section_title = section.get('title', section_id)
                    original_length = len(section.get('content', ''))
                    
                    enhanced_content = self.writer.enhance_section(
                        original_content=section.get('content', ''),
                        vague_points=vague_points,
                        section_title=section_title,
                        progress_info=f"[{idx}/{total_to_deepen}]"
                    )
                    section['content'] = enhanced_content
                    
                    new_length = len(enhanced_content)
                    logger.info(f"ç« èŠ‚æ·±åŒ–å®Œæˆ: {section_title} (+{new_length - original_length} å­—)")
                    break
        
        after_count = _get_content_word_count(state)
        _log_word_count_diff("å†…å®¹æ·±åŒ–", before_count, after_count)
        return state
    
    def _coder_and_artist_node(self, state: SharedState) -> SharedState:
        """ä»£ç å’Œé…å›¾å¹¶è¡Œç”ŸæˆèŠ‚ç‚¹"""
        from concurrent.futures import ThreadPoolExecutor
        
        logger.info("=== Step 5: ä»£ç å’Œé…å›¾å¹¶è¡Œç”Ÿæˆ ===")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ coder å’Œ artist
        # coder ä¿®æ”¹: code_blocks, sections[x].code_ids
        # artist ä¿®æ”¹: images, sections[x].image_ids
        # ä¸¤è€…ä¸å†²çªï¼Œå¯ä»¥å®‰å…¨å¹¶è¡Œ
        
        def run_coder():
            logger.info("â†’ å¼€å§‹ä»£ç ç”Ÿæˆ")
            return self.coder.run(state)
        
        def run_artist():
            logger.info("â†’ å¼€å§‹é…å›¾ç”Ÿæˆï¼ˆå«è¡¥å›¾æ£€æµ‹ï¼‰")
            return self.artist.run(state)
        
        with ThreadPoolExecutor(max_workers=2) as executor:
            coder_future = executor.submit(run_coder)
            artist_future = executor.submit(run_artist)
            
            # ç­‰å¾…ä¸¤è€…å®Œæˆï¼Œå¹¶åˆå¹¶ç»“æœ
            coder_result = coder_future.result()
            artist_result = artist_future.result()
            
            # åˆå¹¶ artist çš„ç»“æœåˆ° stateï¼ˆç‰¹åˆ«æ˜¯ section_imagesï¼‰
            if artist_result:
                if 'section_images' in artist_result:
                    state['section_images'] = artist_result['section_images']
                    logger.info(f"åˆå¹¶ section_images: {len(state['section_images'])} å¼ ")
                if 'images' in artist_result:
                    state['images'] = artist_result['images']
        
        code_count = len(state.get('code_blocks', []))
        image_count = len(state.get('images', []))
        logger.info(f"=== ä»£ç å’Œé…å›¾å¹¶è¡Œç”Ÿæˆå®Œæˆ: {code_count} ä¸ªä»£ç å—, {image_count} å¼ å›¾ç‰‡ ===")
        
        return state
    
    def _reviewer_node(self, state: SharedState) -> SharedState:
        """è´¨é‡å®¡æ ¸èŠ‚ç‚¹"""
        logger.info("=== Step 7: è´¨é‡å®¡æ ¸ ===")
        return self.reviewer.run(state)
    
    def _revision_node(self, state: SharedState) -> SharedState:
        """ä¿®è®¢èŠ‚ç‚¹"""
        logger.info("=== Step 7.1: ä¿®è®¢ ===")
        before_count = _get_content_word_count(state)
        state['revision_count'] = state.get('revision_count', 0) + 1
        
        # æ ¹æ®å®¡æ ¸é—®é¢˜ä¿®è®¢å†…å®¹
        review_issues = state.get('review_issues', [])
        total_issues = len(review_issues)
        target_length = state.get('target_length', 'medium')
        
        # Mini/Short æ¨¡å¼ï¼šæŒ‰ç« èŠ‚åˆ†ç»„é—®é¢˜ï¼Œä½¿ç”¨ correct_sectionï¼ˆåªæ›´æ­£ä¸æ‰©å±•ï¼‰
        if target_length in ('mini', 'short'):
            # æŒ‰ç« èŠ‚åˆ†ç»„é—®é¢˜
            section_issues = {}
            for issue in review_issues:
                section_id = issue.get('section_id', '')
                if section_id not in section_issues:
                    section_issues[section_id] = []
                section_issues[section_id].append({
                    'severity': issue.get('severity', 'medium'),
                    'description': issue.get('description', ''),
                    'affected_content': issue.get('affected_content', '')
                })
            
            # å¯¹æ¯ä¸ªæœ‰é—®é¢˜çš„ç« èŠ‚è¿›è¡Œæ›´æ­£
            for idx, (section_id, issues) in enumerate(section_issues.items(), 1):
                for section in state.get('sections', []):
                    if section.get('id') == section_id:
                        section_title = section.get('title', section_id)
                        corrected_content = self.writer.correct_section(
                            original_content=section.get('content', ''),
                            issues=issues,
                            section_title=section_title,
                            progress_info=f"[{idx}/{len(section_issues)}]"
                        )
                        section['content'] = corrected_content
                        break
        else:
            # å…¶ä»–æ¨¡å¼ï¼šä½¿ç”¨ enhance_sectionï¼ˆå¯æ‰©å±•å†…å®¹ï¼‰
            for idx, issue in enumerate(review_issues, 1):
                section_id = issue.get('section_id', '')
                suggestion = issue.get('suggestion', '')
                
                # æ‰¾åˆ°å¯¹åº”ç« èŠ‚å¹¶ä¿®è®¢
                for section in state.get('sections', []):
                    if section.get('id') == section_id:
                        section_title = section.get('title', section_id)
                        enhanced_content = self.writer.enhance_section(
                            original_content=section.get('content', ''),
                            vague_points=[{
                                'location': section_title,
                                'issue': issue.get('description', ''),
                                'question': suggestion,
                                'suggestion': 'æ ¹æ®å®¡æ ¸å»ºè®®ä¿®æ”¹'
                            }],
                            section_title=section_title,
                            progress_info=f"[{idx}/{total_issues}]"
                        )
                        section['content'] = enhanced_content
                        break
        
        after_count = _get_content_word_count(state)
        _log_word_count_diff("ä¿®è®¢", before_count, after_count)
        return state
    
    def _assembler_node(self, state: SharedState) -> SharedState:
        """æ–‡æ¡£ç»„è£…èŠ‚ç‚¹"""
        logger.info("=== Step 8: æ–‡æ¡£ç»„è£… ===")
        return self.assembler.run(state)
    
    def _should_deepen(self, state: SharedState) -> Literal["deepen", "continue"]:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ·±åŒ–å†…å®¹"""
        if not state.get('all_sections_detailed', True):
            if state.get('questioning_count', 0) < self.max_questioning_rounds:
                return "deepen"
        return "continue"
    
    def _should_revise(self, state: SharedState) -> Literal["revision", "assemble"]:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿®è®¢"""
        target_length = state.get('target_length', 'medium')
        
        # Mini/Short æ¨¡å¼åªå¤„ç† high çº§åˆ«é—®é¢˜ï¼Œä¸”æœ€å¤šä¿®è®¢ 1 è½®
        if target_length in ('mini', 'short'):
            revision_count = state.get('revision_count', 0)
            # Mini æ¨¡å¼æœ€å¤šä¿®è®¢ 1 è½®
            if revision_count >= 1:
                logger.info(f"[{target_length}] æ¨¡å¼ï¼šå·²è¾¾åˆ°æœ€å¤§ä¿®è®¢è½®æ•° (1)ï¼Œè·³è¿‡ä¿®è®¢")
                return "assemble"
            
            review_issues = state.get('review_issues', [])
            high_issues = [i for i in review_issues if i.get('severity') == 'high']
            if high_issues:
                logger.info(f"[{target_length}] æ¨¡å¼ï¼šåªå¤„ç† {len(high_issues)} ä¸ª high çº§åˆ«é—®é¢˜")
                # åªä¿ç•™ high çº§åˆ«é—®é¢˜
                state['review_issues'] = high_issues
                return "revision"
            logger.info(f"[{target_length}] æ¨¡å¼ï¼šæ—  high çº§åˆ«é—®é¢˜ï¼Œè·³è¿‡ä¿®è®¢")
            return "assemble"
        
        if not state.get('review_approved', True):
            if state.get('revision_count', 0) < self.max_revision_rounds:
                return "revision"
        
        logger.info("å®¡æ ¸é€šè¿‡æˆ–ä¿®è®¢å®Œæˆï¼Œè¿›å…¥ç»„è£…")
        return "assemble"

    def _should_refine_search(self, state: SharedState) -> Literal["search", "continue"]:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç»†åŒ–æœç´¢"""
        # Mini/Short æ¨¡å¼è·³è¿‡çŸ¥è¯†å¢å¼ºï¼Œç›´æ¥è¿›å…¥è¿½é—®é˜¶æ®µ
        target_length = state.get('target_length', 'medium')
        if target_length in ('mini', 'short'):
            logger.info(f"[{target_length}] æ¨¡å¼è·³è¿‡çŸ¥è¯†å¢å¼º")
            return "continue"
        
        gaps = state.get('knowledge_gaps', [])
        search_count = state.get('search_count', 0)
        max_count = state.get('max_search_count', 5)
        
        # æœ‰çŸ¥è¯†ç©ºç™½ä¸”æœªè¾¾åˆ°æœç´¢ä¸Šé™
        if gaps and search_count < max_count:
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡è¦çš„ç©ºç™½ï¼ˆmissing_data æˆ– vague_conceptï¼‰
            important_gaps = [g for g in gaps if g.get('gap_type') in ['missing_data', 'vague_concept']]
            if important_gaps:
                logger.info(f"æ£€æµ‹åˆ° {len(important_gaps)} ä¸ªé‡è¦çŸ¥è¯†ç©ºç™½ï¼Œè§¦å‘ç»†åŒ–æœç´¢")
                return "search"
        
        logger.info("æ— éœ€ç»†åŒ–æœç´¢ï¼Œç»§ç»­åˆ°è¿½é—®é˜¶æ®µ")
        return "continue"
    
    def compile(self, checkpointer=None):
        """
        ç¼–è¯‘å·¥ä½œæµ
        
        Args:
            checkpointer: æ£€æŸ¥ç‚¹å­˜å‚¨ (å¯é€‰)
        """
        if checkpointer is None:
            checkpointer = MemorySaver()
        
        self.app = self.workflow.compile(checkpointer=checkpointer)
        return self.app
    
    def generate(
        self,
        topic: str,
        article_type: str = "tutorial",
        target_audience: str = "intermediate",
        target_length: str = "medium",
        source_material: str = None,
        on_progress: Callable[[str, str], None] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆåšå®¢
        
        Args:
            topic: æŠ€æœ¯ä¸»é¢˜
            article_type: æ–‡ç« ç±»å‹
            target_audience: ç›®æ ‡å—ä¼—
            target_length: ç›®æ ‡é•¿åº¦
            source_material: å‚è€ƒèµ„æ–™
            on_progress: è¿›åº¦å›è°ƒ
            
        Returns:
            ç”Ÿæˆç»“æœ
        """
        if self.app is None:
            self.compile()
        
        # åˆ›å»ºåˆå§‹çŠ¶æ€
        initial_state = create_initial_state(
            topic=topic,
            article_type=article_type,
            target_audience=target_audience,
            target_length=target_length,
            source_material=source_material
        )
        
        logger.info(f"å¼€å§‹ç”Ÿæˆåšå®¢: {topic}")
        logger.info(f"  ç±»å‹: {article_type}, å—ä¼—: {target_audience}, é•¿åº¦: {target_length}")
        
        # æ‰§è¡Œå·¥ä½œæµ
        config = {"configurable": {"thread_id": f"blog_{topic}"}}
        
        try:
            final_state = self.app.invoke(initial_state, config)
            
            logger.info("åšå®¢ç”Ÿæˆå®Œæˆ!")
            
            return {
                "success": True,
                "markdown": final_state.get('final_markdown', ''),
                "outline": final_state.get('outline', {}),
                "sections_count": len(final_state.get('sections', [])),
                "images_count": len(final_state.get('images', [])),
                "code_blocks_count": len(final_state.get('code_blocks', [])),
                "review_score": final_state.get('review_score', 0),
                "error": None
            }
            
        except Exception as e:
            logger.error(f"åšå®¢ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "markdown": "",
                "error": str(e)
            }
    
    async def generate_stream(
        self,
        topic: str,
        article_type: str = "tutorial",
        target_audience: str = "intermediate",
        target_length: str = "medium",
        source_material: str = None
    ):
        """
        æµå¼ç”Ÿæˆåšå®¢ (å¼‚æ­¥ç”Ÿæˆå™¨)
        
        Args:
            topic: æŠ€æœ¯ä¸»é¢˜
            article_type: æ–‡ç« ç±»å‹
            target_audience: ç›®æ ‡å—ä¼—
            target_length: ç›®æ ‡é•¿åº¦
            source_material: å‚è€ƒèµ„æ–™
            
        Yields:
            ç”Ÿæˆè¿›åº¦å’Œä¸­é—´ç»“æœ
        """
        if self.app is None:
            self.compile()
        
        initial_state = create_initial_state(
            topic=topic,
            article_type=article_type,
            target_audience=target_audience,
            target_length=target_length,
            source_material=source_material
        )
        
        config = {"configurable": {"thread_id": f"blog_{topic}"}}
        
        # ä½¿ç”¨ stream æ–¹æ³•è·å–ä¸­é—´çŠ¶æ€
        for event in self.app.stream(initial_state, config):
            for node_name, state in event.items():
                yield {
                    "stage": node_name,
                    "state": state
                }
