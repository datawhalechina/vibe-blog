"""
æ™ºèƒ½çŸ¥è¯†æºæœç´¢æœåŠ¡ - æ ¹æ®ä¸»é¢˜æ™ºèƒ½è·¯ç”±åˆ°ä¸åŒæœç´¢æº
"""

import json
import logging
import os
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from .search_service import get_search_service
from .arxiv_service import get_arxiv_service

logger = logging.getLogger(__name__)

# ä¸“ä¸šåšå®¢ç½‘ç«™é…ç½®
PROFESSIONAL_BLOGS = {
    'langchain': {
        'site': 'blog.langchain.dev',
        'name': 'LangChain Blog',
        'keywords': ['langchain', 'langgraph', 'lcel', 'langsmith'],
        'quality_weight': 0.85,
    },
    'anthropic': {
        'site': 'anthropic.com',
        'name': 'Anthropic Research',
        'keywords': ['claude', 'anthropic', 'constitutional ai', 'rlhf'],
        'quality_weight': 0.95,
    },
    'openai': {
        'site': 'openai.com',
        'name': 'OpenAI Blog',
        'keywords': ['gpt', 'chatgpt', 'openai', 'dall-e', 'whisper', 'sora'],
        'quality_weight': 0.95,
    },
    'huggingface': {
        'site': 'huggingface.co',
        'name': 'Hugging Face',
        'keywords': ['huggingface', 'transformers', 'diffusers', 'å¼€æºæ¨¡å‹', 'llama', 'mistral'],
        'quality_weight': 0.85,
    },
    'jiqizhixin': {
        'site': 'jiqizhixin.com',
        'name': 'æœºå™¨ä¹‹å¿ƒ',
        'keywords': ['æœºå™¨ä¹‹å¿ƒ', 'ä¸­æ–‡', 'aièµ„è®¯'],
        'quality_weight': 0.70,
    },
    'github': {
        'site': 'github.com',
        'name': 'GitHub',
        'keywords': ['github', 'å¼€æº', 'repo', 'ä»“åº“', 'æºç '],
        'quality_weight': 0.75,
    },
    'google_ai': {
        'site': 'blog.google/technology/ai',
        'name': 'Google AI Blog',
        'keywords': ['google', 'gemini', 'bard', 'deepmind', 'tensorflow', 'jax'],
        'quality_weight': 0.90,
    },
    'devto': {
        'site': 'dev.to',
        'name': 'Dev.to',
        'keywords': ['dev.to', 'ç¤¾åŒº', 'tutorial'],
        'quality_weight': 0.70,
    },
    'stackoverflow': {
        'site': 'stackoverflow.com',
        'name': 'Stack Overflow',
        'keywords': ['stackoverflow', 'é—®ç­”', 'debug', 'æŠ¥é”™', 'error'],
        'quality_weight': 0.75,
    },
    'aws': {
        'site': 'aws.amazon.com/blogs',
        'name': 'AWS Blog',
        'keywords': ['aws', 'lambda', 'sagemaker', 'bedrock', 's3', 'ec2'],
        'quality_weight': 0.80,
    },
    'microsoft': {
        'site': 'devblogs.microsoft.com',
        'name': 'Microsoft DevBlogs',
        'keywords': ['azure', 'microsoft', 'copilot', '.net', 'typescript', 'vscode'],
        'quality_weight': 0.80,
    },
    # ===== 71 å·æ–¹æ¡ˆæ–°å¢ AI æƒå¨åšå®¢æº =====
    'deepmind': {
        'site': 'deepmind.google',
        'name': 'Google DeepMind',
        'keywords': ['deepmind', 'alphafold', 'alphacode', 'gemma', 'deepmind research'],
        'quality_weight': 0.95,
    },
    'meta_ai': {
        'site': 'ai.meta.com',
        'name': 'Meta AI',
        'keywords': ['meta ai', 'llama', 'llama3', 'codellama', 'meta research', 'fair'],
        'quality_weight': 0.95,
    },
    'mistral': {
        'site': 'mistral.ai',
        'name': 'Mistral AI',
        'keywords': ['mistral', 'mixtral', 'mistral ai', 'pixtral', 'codestral'],
        'quality_weight': 0.90,
    },
    'xai': {
        'site': 'x.ai',
        'name': 'xAI',
        'keywords': ['xai', 'grok', 'x.ai'],
        'quality_weight': 0.85,
    },
    'ms_research': {
        'site': 'microsoft.com/research',
        'name': 'Microsoft Research',
        'keywords': ['microsoft research', 'phi', 'orca', 'autogen', 'semantic kernel'],
        'quality_weight': 0.90,
    },
    # ===== 71 å·æ–¹æ¡ˆ Phase B: ç¤¾åŒºæœç´¢æº =====
    'reddit_ai': {
        'site': 'reddit.com',
        'name': 'Reddit AI',
        'keywords': ['reddit', 'ç¤¾åŒºè®¨è®º', 'r/machinelearning', 'r/localllama'],
        'quality_weight': 0.70,
    },
    'hackernews': {
        'site': 'news.ycombinator.com',
        'name': 'Hacker News',
        'keywords': ['hacker news', 'hn', 'ycombinator', 'hackernews'],
        'quality_weight': 0.75,
    },
}

# ===== 71 å·æ–¹æ¡ˆ Phase C: AI è¯é¢˜è‡ªåŠ¨å¢å¼º =====

# AI è¯é¢˜å…³é”®è¯ï¼ˆè§¦å‘è‡ªåŠ¨å¢å¼ºæœç´¢ï¼‰
AI_TOPIC_KEYWORDS = [
    # é€šç”¨ AI æœ¯è¯­
    'ai', 'äººå·¥æ™ºèƒ½', 'artificial intelligence', 'machine learning', 'æœºå™¨å­¦ä¹ ',
    'deep learning', 'æ·±åº¦å­¦ä¹ ', 'neural network', 'ç¥ç»ç½‘ç»œ',
    # LLM ç›¸å…³
    'llm', 'å¤§æ¨¡å‹', 'å¤§è¯­è¨€æ¨¡å‹', 'large language model', 'foundation model',
    'prompt', 'rag', 'agent', 'fine-tuning', 'å¾®è°ƒ', 'embedding',
    # å…·ä½“æ¨¡å‹/äº§å“
    'gpt', 'claude', 'gemini', 'llama', 'mistral', 'grok',
    'chatgpt', 'copilot', 'cursor', 'midjourney', 'stable diffusion',
    # AI åº”ç”¨
    'ai agent', 'ai coding', 'vibe coding', 'ai ç¼–ç¨‹', 'ai å†™ä½œ',
    'mcp', 'model context protocol',
]

# AI è¯é¢˜è‡ªåŠ¨å¢å¼ºçš„æœç´¢æº
AI_BOOST_SOURCES = [
    'anthropic', 'openai', 'google_ai', 'deepmind',
    'meta_ai', 'mistral', 'huggingface',
]

# å…¨å±€æœåŠ¡å®ä¾‹
_smart_search_service: Optional['SmartSearchService'] = None


class SmartSearchService:
    """
    æ™ºèƒ½æœç´¢æœåŠ¡ - æ ¹æ®ä¸»é¢˜æ™ºèƒ½é€‰æ‹©æœç´¢æº
    """
    
    def __init__(self, llm_client=None):
        """
        åˆå§‹åŒ–æ™ºèƒ½æœç´¢æœåŠ¡

        Args:
            llm_client: LLM å®¢æˆ·ç«¯ï¼Œç”¨äºæ™ºèƒ½è·¯ç”±
        """
        self.llm = llm_client
        self.max_workers = int(os.environ.get('BLOG_GENERATOR_MAX_WORKERS', '3'))
        # 37.04: æŸ¥è¯¢é‡å¤æ£€æµ‹
        from utils.query_deduplicator import QueryDeduplicator
        self.deduplicator = QueryDeduplicator()
        # 71: SourceCurator æºè´¨é‡è¯„ä¼°ä¸å¥åº·æ£€æŸ¥
        from .source_curator import SourceCurator
        self.curator = SourceCurator()
    
    def search(self, topic: str, article_type: str = '', max_results_per_source: int = 5) -> Dict[str, Any]:
        """
        æ™ºèƒ½æœç´¢ - æ ¹æ®ä¸»é¢˜é€‰æ‹©æœç´¢æºå¹¶å¹¶è¡Œæ‰§è¡Œ
        
        Args:
            topic: æœç´¢ä¸»é¢˜
            article_type: æ–‡ç« ç±»å‹
            max_results_per_source: æ¯ä¸ªæºçš„æœ€å¤§ç»“æœæ•°
            
        Returns:
            åˆå¹¶åçš„æœç´¢ç»“æœ
        """
        logger.info(f"ğŸ§  æ™ºèƒ½æœç´¢å¼€å§‹: {topic}")

        # 37.04: æŸ¥è¯¢é‡å¤æ£€æµ‹
        if self.deduplicator.is_duplicate(topic, agent="smart_search"):
            logger.warning(f"ğŸ” é‡å¤æŸ¥è¯¢è·³è¿‡: {topic}")
            allowed = self.deduplicator.rollback()
            return {
                'success': True,
                'results': [],
                'summary': '',
                'sources_used': [],
                'error': None,
                'skipped_duplicate': True,
                'rollback_allowed': allowed,
            }
        self.deduplicator.record(topic, agent="smart_search")
        self.deduplicator.reset_rollback_count()

        # ç¬¬ä¸€æ­¥ï¼šLLM åˆ¤æ–­éœ€è¦å“ªäº›æœç´¢æº
        routing_result = self._route_search_sources(topic)
        
        sources = routing_result.get('sources', ['general'])
        arxiv_query = routing_result.get('arxiv_query', topic)
        blog_query = routing_result.get('blog_query', topic)

        # 71 å·æ–¹æ¡ˆ Phase Cï¼šAI è¯é¢˜è‡ªåŠ¨å¢å¼º
        if os.environ.get('AI_BOOST_ENABLED', 'true').lower() == 'true':
            sources = self._boost_ai_sources(sources, topic)

        logger.info(f"ğŸ§  æœç´¢æºè·¯ç”±ç»“æœ: {sources}")

        # 71: å¥åº·æ£€æŸ¥ â€” è¿‡æ»¤ä¸å¥åº·çš„æº
        sources = self.curator.get_healthy_sources(sources)

        # ç¬¬äºŒæ­¥ï¼šå¹¶è¡Œæ‰§è¡Œæœç´¢
        all_results = []
        search_tasks = []
        
        # å‡†å¤‡æœç´¢ä»»åŠ¡
        if 'arxiv' in sources:
            search_tasks.append(('arxiv', arxiv_query))
        
        # ä¸“ä¸šåšå®¢æœç´¢
        for source in sources:
            if source in PROFESSIONAL_BLOGS:
                search_tasks.append(('blog', source, blog_query))
        
        # é€šç”¨æœç´¢ï¼ˆå§‹ç»ˆåŒ…å«ï¼‰
        if 'general' in sources or not search_tasks:
            search_tasks.append(('general', blog_query))

        # Google æœç´¢ï¼ˆ75.02 Serperï¼‰
        if 'google' in sources:
            search_tasks.append(('google', blog_query))

        # æœç‹—æœç´¢ï¼ˆ75.07 è…¾è®¯äº‘ SearchProï¼‰
        if 'sogou' in sources:
            search_tasks.append(('sogou', blog_query))
        
        # å¹¶è¡Œæ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {}
            
            for task in search_tasks:
                if task[0] == 'arxiv':
                    future = executor.submit(self._search_arxiv, task[1], max_results_per_source)
                    futures[future] = 'arxiv'
                elif task[0] == 'blog':
                    future = executor.submit(self._search_blog, task[1], task[2], max_results_per_source)
                    futures[future] = f'blog:{task[1]}'
                elif task[0] == 'general':
                    future = executor.submit(self._search_general, task[1], max_results_per_source)
                    futures[future] = 'general'
                elif task[0] == 'google':
                    future = executor.submit(self._search_google, task[1], max_results_per_source)
                    futures[future] = 'google'
                elif task[0] == 'sogou':
                    future = executor.submit(self._search_sogou, task[1], max_results_per_source)
                    futures[future] = 'sogou'
            
            for future in as_completed(futures):
                source_name = futures[future]
                try:
                    result = future.result()
                    if result.get('success') and result.get('results'):
                        all_results.extend(result['results'])
                        logger.info(f"âœ… {source_name} æœç´¢å®Œæˆ: {len(result['results'])} æ¡ç»“æœ")
                        # 71: è®°å½•æˆåŠŸ
                        self.curator.record_success(source_name.replace('blog:', ''))
                    elif not result.get('success'):
                        # 71: è®°å½•å¤±è´¥
                        self.curator.record_failure(source_name.replace('blog:', ''))
                except Exception as e:
                    logger.error(f"âŒ {source_name} æœç´¢å¤±è´¥: {e}")
                    # 71: è®°å½•å¤±è´¥
                    self.curator.record_failure(source_name.replace('blog:', ''))
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶å»é‡
        merged_results = self._merge_and_dedupe(all_results)
        
        logger.info(f"ğŸ§  æ™ºèƒ½æœç´¢å®Œæˆ: å…± {len(merged_results)} æ¡ç»“æœ")
        
        return {
            'success': True,
            'results': merged_results,
            'summary': self._generate_summary(merged_results),
            'sources_used': sources,
            'error': None
        }
    
    def _route_search_sources(self, topic: str) -> Dict[str, Any]:
        """ä½¿ç”¨ LLM åˆ¤æ–­éœ€è¦å“ªäº›æœç´¢æº"""
        if not self.llm:
            return self._rule_based_routing(topic)

        from ..prompts import get_prompt_manager
        prompt = get_prompt_manager().render_search_router(topic)

        try:
            response = self.llm.chat(
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )

            result = self._extract_json(response)

            # ç¡®ä¿ general å§‹ç»ˆåŒ…å«
            if 'general' not in result.get('sources', []):
                result['sources'].append('general')

            return result

        except Exception as e:
            logger.warning(f"LLM è·¯ç”±å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åŒ¹é…: {e}")
            return self._rule_based_routing(topic)

    @staticmethod
    def _extract_json(text: str) -> dict:
        """ä» LLM å“åº”ä¸­æå– JSONï¼ˆå¤„ç† markdown åŒ…è£¹ï¼‰"""
        text = text.strip()
        if '```json' in text:
            start = text.find('```json') + 7
            end = text.find('```', start)
            if end != -1:
                text = text[start:end].strip()
            else:
                text = text[start:].strip()
        elif '```' in text:
            start = text.find('```') + 3
            end = text.find('```', start)
            if end != -1:
                text = text[start:end].strip()
            else:
                text = text[start:].strip()
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            return json.loads(text, strict=False)
    
    def _rule_based_routing(self, topic: str) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„ç®€å•è·¯ç”±ï¼ˆLLM ä¸å¯ç”¨æ—¶çš„å¤‡é€‰ï¼‰"""
        topic_lower = topic.lower()
        sources = ['general']
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ arXiv
        arxiv_keywords = ['è®ºæ–‡', 'paper', 'ç ”ç©¶', 'research', 'ç®—æ³•', 'algorithm', 'æ¨¡å‹', 'model', 'transformer', 'attention']
        if any(kw in topic_lower for kw in arxiv_keywords):
            sources.append('arxiv')
        
        # æ£€æŸ¥ä¸“ä¸šåšå®¢
        for blog_id, config in PROFESSIONAL_BLOGS.items():
            if any(kw in topic_lower for kw in config['keywords']):
                sources.append(blog_id)

        # 75.02: å¦‚æœ Serper å¯ç”¨ï¼Œè‡ªåŠ¨åŠ å…¥ Google æœç´¢
        try:
            from .serper_search_service import get_serper_service
            serper = get_serper_service()
            if serper and serper.is_available():
                sources.append('google')
        except Exception:
            pass

        # 75.07: å¦‚æœæœç‹—å¯ç”¨ä¸”ä¸ºä¸­æ–‡ä¸»é¢˜ï¼Œè‡ªåŠ¨åŠ å…¥æœç‹—æœç´¢
        try:
            from .sogou_search_service import get_sogou_service
            sogou = get_sogou_service()
            if sogou and sogou.is_available():
                has_chinese = any('\u4e00' <= c <= '\u9fff' for c in topic)
                if has_chinese:
                    sources.append('sogou')
        except Exception:
            pass
        
        return {
            'sources': sources,
            'arxiv_query': topic,
            'blog_query': topic
        }

    # ===== 71 å·æ–¹æ¡ˆ Phase C: AI è¯é¢˜è‡ªåŠ¨å¢å¼º =====

    @staticmethod
    def _is_ai_topic(topic: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸º AI ç›¸å…³è¯é¢˜"""
        topic_lower = topic.lower()
        return any(kw in topic_lower for kw in AI_TOPIC_KEYWORDS)

    def _boost_ai_sources(self, sources: List[str], topic: str) -> List[str]:
        """AI è¯é¢˜è‡ªåŠ¨å¢å¼ºï¼šç¡®ä¿è¦†ç›–æ‰€æœ‰ AI æƒå¨åšå®¢æº"""
        if not self._is_ai_topic(topic):
            return sources

        boosted = list(sources)
        added = 0
        for src in AI_BOOST_SOURCES:
            if src not in boosted:
                boosted.append(src)
                added += 1

        # AI è¯é¢˜ä¹ŸåŠ å…¥ arXiv
        if 'arxiv' not in boosted:
            boosted.append('arxiv')
            added += 1

        if added:
            logger.info(f"ğŸš€ AI è¯é¢˜å¢å¼º: +{added} ä¸ªé¢å¤–æº")
        return boosted

    def _search_arxiv(self, query: str, max_results: int) -> Dict[str, Any]:
        """æœç´¢ arXiv"""
        arxiv_service = get_arxiv_service()
        if arxiv_service:
            return arxiv_service.search(query, max_results)
        return {'success': False, 'results': [], 'error': 'arXiv æœåŠ¡ä¸å¯ç”¨'}
    
    def _search_blog(self, blog_id: str, query: str, max_results: int) -> Dict[str, Any]:
        """æœç´¢ä¸“ä¸šåšå®¢ï¼ˆä½¿ç”¨ site: é™å®šï¼‰"""
        search_service = get_search_service()
        if not search_service or not search_service.is_available():
            return {'success': False, 'results': [], 'error': 'æœç´¢æœåŠ¡ä¸å¯ç”¨'}
        
        blog_config = PROFESSIONAL_BLOGS.get(blog_id)
        if not blog_config:
            return {'success': False, 'results': [], 'error': f'æœªçŸ¥åšå®¢: {blog_id}'}
        
        # ä½¿ç”¨ site: é™å®šæœç´¢
        site_query = f"{query} site:{blog_config['site']}"
        logger.info(f"ğŸ“ ä¸“ä¸šåšå®¢æœç´¢: {site_query}")
        
        result = search_service.search(site_query, max_results)
        
        # æ ‡è®°æ¥æº
        if result.get('results'):
            for item in result['results']:
                item['source'] = blog_config['name']
        
        return result
    
    def _search_general(self, query: str, max_results: int) -> Dict[str, Any]:
        """é€šç”¨æœç´¢"""
        search_service = get_search_service()
        if search_service and search_service.is_available():
            result = search_service.search(query, max_results)
            # æ ‡è®°æ¥æº
            if result.get('results'):
                for item in result['results']:
                    if not item.get('source'):
                        item['source'] = 'é€šç”¨æœç´¢'
            return result
        return {'success': False, 'results': [], 'error': 'æœç´¢æœåŠ¡ä¸å¯ç”¨'}
    
    def _search_google(self, query: str, max_results: int) -> Dict[str, Any]:
        """Google æœç´¢ï¼ˆé€šè¿‡ Serper APIï¼Œ75.02ï¼‰"""
        from .serper_search_service import get_serper_service
        serper = get_serper_service()
        if not serper or not serper.is_available():
            return {'success': False, 'results': [], 'error': 'Serper æœåŠ¡ä¸å¯ç”¨'}
        return serper.search(query, max_results)

    def _search_sogou(self, query: str, max_results: int) -> Dict[str, Any]:
        """æœç‹—æœç´¢ï¼ˆé€šè¿‡è…¾è®¯äº‘ SearchPro APIï¼Œ75.07ï¼‰"""
        from .sogou_search_service import get_sogou_service
        sogou = get_sogou_service()
        if not sogou or not sogou.is_available():
            return {'success': False, 'results': [], 'error': 'æœç‹—æœç´¢æœåŠ¡ä¸å¯ç”¨'}
        return sogou.search(query, max_results)

    def _merge_and_dedupe(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆå¹¶å»é‡æœç´¢ç»“æœï¼Œå¹¶æŒ‰æºè´¨é‡æ’åº"""
        seen_urls = set()
        merged = []

        for item in results:
            url = item.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                merged.append(item)
            elif not url:
                # æ—  URL çš„ç»“æœä¹Ÿä¿ç•™ï¼ˆå¦‚æŸäº›æ‘˜è¦ï¼‰
                merged.append(item)

        # 71: SourceCurator æŒ‰æºè´¨é‡æ’åº
        return self.curator.rank(merged)
    
    def _generate_summary(self, results: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæœç´¢ç»“æœæ‘˜è¦"""
        if not results:
            return ''
        
        summary_parts = []
        for i, item in enumerate(results, 1):
            source = item.get('source', 'æœªçŸ¥æ¥æº')
            title = item.get('title', '')
            content = item.get('content', '')[:800]
            
            summary_parts.append(f"[{source}] {title}\n{content}")
        
        return '\n\n---\n\n'.join(summary_parts)


def init_smart_search_service(llm_client=None) -> SmartSearchService:
    """åˆå§‹åŒ–æ™ºèƒ½æœç´¢æœåŠ¡"""
    global _smart_search_service
    _smart_search_service = SmartSearchService(llm_client)
    logger.info("æ™ºèƒ½çŸ¥è¯†æºæœç´¢æœåŠ¡å·²åˆå§‹åŒ–")
    return _smart_search_service


def get_smart_search_service() -> Optional[SmartSearchService]:
    """è·å–æ™ºèƒ½æœç´¢æœåŠ¡å®ä¾‹"""
    return _smart_search_service
